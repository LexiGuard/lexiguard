{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling LSTM, GRADIENT BOOSTING, FAST TEXT + RFC, FAST TEXT + LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImportS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import spacy\n",
    "import ast\n",
    "import joblib\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this initialize tqdm which is useful to show a progress bar when applying operations in a pandas df\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/undersampled_data_60_40_FINAL.csv')\n",
    "\n",
    "'''Column description:\n",
    "-------------------\n",
    "\n",
    "'raw'\n",
    "raw original comment, no cleaing or preprocessing whatsoever\n",
    "\n",
    "'clean'\n",
    "previous column with regex cleaning (HTML anchor tags, URLs, newlines etc.)\n",
    "\n",
    "'clean_pp'\n",
    "previous column with spaCy preprocessing (tokenization, punctuation removal, make lower case)\n",
    "\n",
    "'clean_pp_lemma'\n",
    "previous column with spaCy lemmatization\n",
    "\n",
    "'clean_pp_lemma_stop'\n",
    "previous column with stop words removed\n",
    "\n",
    "'toxic'\n",
    "target/label'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_pp</th>\n",
       "      <th>clean_pp_lemma</th>\n",
       "      <th>clean_pp_lemma_stop</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, what are the chances he will turn out to...</td>\n",
       "      <td>Well, what are the chances he will turn out to...</td>\n",
       "      <td>well what are the chances he will turn out to ...</td>\n",
       "      <td>well what be the chance he will turn out to ha...</td>\n",
       "      <td>chance turn active proponent slavery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The moment of critical mass is approaching whe...</td>\n",
       "      <td>The moment of critical mass is approaching whe...</td>\n",
       "      <td>the moment of critical mass is approaching whe...</td>\n",
       "      <td>the moment of critical mass be approach when t...</td>\n",
       "      <td>moment critical mass approach deed gupta co li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Hey listen to me,\" he said. \"I'm not going to...</td>\n",
       "      <td>\"Hey listen to me,\" he said. \"I'm not going to...</td>\n",
       "      <td>hey listen to me he said i 'm not going to put...</td>\n",
       "      <td>hey listen to i he say i be not go to put up w...</td>\n",
       "      <td>hey listen say go crap prove reporter say uh a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are already owed $488 M plus interest($2Bil...</td>\n",
       "      <td>We are already owed $ M plus interest($ Billio...</td>\n",
       "      <td>we are already owed $ m plus interest($ billio...</td>\n",
       "      <td>we be already owe $ m plus interest($ billion ...</td>\n",
       "      <td>owe $ m plus interest($ billion audits state c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There is a reason there are no teeth to the la...</td>\n",
       "      <td>There is a reason there are no teeth to the la...</td>\n",
       "      <td>there is a reason there are no teeth to the la...</td>\n",
       "      <td>there be a reason there be no tooth to the law...</td>\n",
       "      <td>reason tooth law unlawful law way force free e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360296</th>\n",
       "      <td>Do you still beat your wife? Simple question.</td>\n",
       "      <td>Do you still beat your wife? Simple question.</td>\n",
       "      <td>do you still beat your wife simple question</td>\n",
       "      <td>do you still beat your wife simple question</td>\n",
       "      <td>beat wife simple question</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360297</th>\n",
       "      <td>The fascist dictator continues the insanity ag...</td>\n",
       "      <td>The fascist dictator continues the insanity ag...</td>\n",
       "      <td>the fascist dictator continues the insanity ag...</td>\n",
       "      <td>the fascist dictator continue the insanity aga...</td>\n",
       "      <td>fascist dictator continue insanity human civil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360298</th>\n",
       "      <td>Sean Hannity is a lightweight foolish commenta...</td>\n",
       "      <td>Sean Hannity is a lightweight foolish commenta...</td>\n",
       "      <td>sean hannity is a lightweight foolish commenta...</td>\n",
       "      <td>sean hannity be a lightweight foolish commenta...</td>\n",
       "      <td>sean hannity lightweight foolish commentator f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360299</th>\n",
       "      <td>There are a number of countries which make it ...</td>\n",
       "      <td>There are a number of countries which make it ...</td>\n",
       "      <td>there are a number of countries which make it ...</td>\n",
       "      <td>there be a number of country which make it imp...</td>\n",
       "      <td>number country impossible national citizenship...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360300</th>\n",
       "      <td>Where's data strategy to track % of foreign bu...</td>\n",
       "      <td>Where's data strategy to track % of foreign bu...</td>\n",
       "      <td>where 's data strategy to track of foreign buyers</td>\n",
       "      <td>where 's datum strategy to track of foreign buyer</td>\n",
       "      <td>datum strategy track foreign buyer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360065 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      raw  \\\n",
       "0       Well, what are the chances he will turn out to...   \n",
       "1       The moment of critical mass is approaching whe...   \n",
       "2       \"Hey listen to me,\" he said. \"I'm not going to...   \n",
       "3       We are already owed $488 M plus interest($2Bil...   \n",
       "4       There is a reason there are no teeth to the la...   \n",
       "...                                                   ...   \n",
       "360296      Do you still beat your wife? Simple question.   \n",
       "360297  The fascist dictator continues the insanity ag...   \n",
       "360298  Sean Hannity is a lightweight foolish commenta...   \n",
       "360299  There are a number of countries which make it ...   \n",
       "360300  Where's data strategy to track % of foreign bu...   \n",
       "\n",
       "                                                    clean  \\\n",
       "0       Well, what are the chances he will turn out to...   \n",
       "1       The moment of critical mass is approaching whe...   \n",
       "2       \"Hey listen to me,\" he said. \"I'm not going to...   \n",
       "3       We are already owed $ M plus interest($ Billio...   \n",
       "4       There is a reason there are no teeth to the la...   \n",
       "...                                                   ...   \n",
       "360296      Do you still beat your wife? Simple question.   \n",
       "360297  The fascist dictator continues the insanity ag...   \n",
       "360298  Sean Hannity is a lightweight foolish commenta...   \n",
       "360299  There are a number of countries which make it ...   \n",
       "360300  Where's data strategy to track % of foreign bu...   \n",
       "\n",
       "                                                 clean_pp  \\\n",
       "0       well what are the chances he will turn out to ...   \n",
       "1       the moment of critical mass is approaching whe...   \n",
       "2       hey listen to me he said i 'm not going to put...   \n",
       "3       we are already owed $ m plus interest($ billio...   \n",
       "4       there is a reason there are no teeth to the la...   \n",
       "...                                                   ...   \n",
       "360296        do you still beat your wife simple question   \n",
       "360297  the fascist dictator continues the insanity ag...   \n",
       "360298  sean hannity is a lightweight foolish commenta...   \n",
       "360299  there are a number of countries which make it ...   \n",
       "360300  where 's data strategy to track of foreign buyers   \n",
       "\n",
       "                                           clean_pp_lemma  \\\n",
       "0       well what be the chance he will turn out to ha...   \n",
       "1       the moment of critical mass be approach when t...   \n",
       "2       hey listen to i he say i be not go to put up w...   \n",
       "3       we be already owe $ m plus interest($ billion ...   \n",
       "4       there be a reason there be no tooth to the law...   \n",
       "...                                                   ...   \n",
       "360296        do you still beat your wife simple question   \n",
       "360297  the fascist dictator continue the insanity aga...   \n",
       "360298  sean hannity be a lightweight foolish commenta...   \n",
       "360299  there be a number of country which make it imp...   \n",
       "360300  where 's datum strategy to track of foreign buyer   \n",
       "\n",
       "                                      clean_pp_lemma_stop  toxic  \n",
       "0                    chance turn active proponent slavery      0  \n",
       "1       moment critical mass approach deed gupta co li...      0  \n",
       "2       hey listen say go crap prove reporter say uh a...      1  \n",
       "3       owe $ m plus interest($ billion audits state c...      0  \n",
       "4       reason tooth law unlawful law way force free e...      0  \n",
       "...                                                   ...    ...  \n",
       "360296                          beat wife simple question      0  \n",
       "360297  fascist dictator continue insanity human civil...      1  \n",
       "360298  sean hannity lightweight foolish commentator f...      0  \n",
       "360299  number country impossible national citizenship...      0  \n",
       "360300                 datum strategy track foreign buyer      0  \n",
       "\n",
       "[360065 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raw', 'clean', 'clean_pp', 'clean_pp_lemma', 'clean_pp_lemma_stop',\n",
       "       'toxic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NaNs from df['stopwords_punct_lemma']\n",
    "df.dropna(subset=['clean_pp_lemma'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe that will include the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "def evaluate_model(model, X_train,y_train,X_test,y_test, model_name=\"\", parameters='', comments=''):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    duration = time.time() - start_time\n",
    "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
    "    predicted_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics using probabilities\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predicted_probs)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Create a dictionary including the results\n",
    "    results = {\n",
    "        'Name': model_name if model_name else model.__class__.__name__,\n",
    "        'Parameters': parameters,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Training Time': duration_format,\n",
    "        'Comments': comments\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_pp_lemma'].values\n",
    "y = df['toxic'].values \n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Tokenize and convert text to sequences\n",
    "max_words = 10000  # Set the maximum number of words to consider\n",
    "max_len = 100  # Set the maximum length of each sequence\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer to a file\n",
    "tokenizer_file_path = 'data/tokenizer_andre_lstm.pkl'\n",
    "with open(tokenizer_file_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7877/7877 [==============================] - 342s 43ms/step - loss: 0.3292 - accuracy: 0.8617 - val_loss: 0.2998 - val_accuracy: 0.8769\n",
      "Epoch 2/5\n",
      "7877/7877 [==============================] - 328s 42ms/step - loss: 0.2796 - accuracy: 0.8841 - val_loss: 0.2935 - val_accuracy: 0.8783\n",
      "Epoch 3/5\n",
      "7877/7877 [==============================] - 347s 44ms/step - loss: 0.2536 - accuracy: 0.8951 - val_loss: 0.2979 - val_accuracy: 0.8784\n",
      "Epoch 4/5\n",
      "7877/7877 [==============================] - 342s 43ms/step - loss: 0.2255 - accuracy: 0.9070 - val_loss: 0.3260 - val_accuracy: 0.8728\n",
      "Epoch 5/5\n",
      "7877/7877 [==============================] - 352s 45ms/step - loss: 0.1958 - accuracy: 0.9196 - val_loss: 0.3396 - val_accuracy: 0.8633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1080ca690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open('model5_andre_lstm.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model weights\n",
    "model.save_weights('model_weights5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('lstm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2251/2251 [==============================] - 24s 11ms/step\n",
      "2251/2251 [==============================] - 25s 11ms/step\n",
      "Accuracy: 0.8663991223806813\n",
      "Precision: 0.844780711168348\n",
      "Recall: 0.8172128594663806\n",
      "F1 Score: 0.8307681483175318\n",
      "AUC-ROC: 0.9333377455640817\n",
      "Confusion Matrix:\n",
      "[[38777  4339]\n",
      " [ 5282 23615]]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict(X_test_padded))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee5ce8695951d3743acb1574d7cbc518a435066b16546d59d44a9748127e061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
