{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImportS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import spacy\n",
    "import ast\n",
    "import joblib\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/undersampled_data_60_40.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'toxic', 'stopwords_punct_lemma', 'vector_spacy',\n",
       "       'pos_tags', 'pos_tags_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NaNs from df['stopwords_punct_lemma']\n",
    "df.dropna(subset=['stopwords_punct_lemma'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment_text'].values\n",
    "y = df['toxic'].values \n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Tokenize and convert text to sequences\n",
    "max_words = 10000  # Set the maximum number of words to consider\n",
    "max_len = 100  # Set the maximum length of each sequence\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer to a file\n",
    "tokenizer_file_path = 'data/tokenizer.pkl'\n",
    "with open(tokenizer_file_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9021/9021 [==============================] - 247s 27ms/step - loss: 0.3341 - accuracy: 0.8596 - val_loss: 0.3001 - val_accuracy: 0.8755\n",
      "Epoch 2/5\n",
      "9021/9021 [==============================] - 239s 26ms/step - loss: 0.2868 - accuracy: 0.8808 - val_loss: 0.2987 - val_accuracy: 0.8756\n",
      "Epoch 3/5\n",
      "9021/9021 [==============================] - 248s 27ms/step - loss: 0.2627 - accuracy: 0.8915 - val_loss: 0.3014 - val_accuracy: 0.8744\n",
      "Epoch 4/5\n",
      "9021/9021 [==============================] - 243s 27ms/step - loss: 0.2357 - accuracy: 0.9026 - val_loss: 0.3127 - val_accuracy: 0.8720\n",
      "Epoch 5/5\n",
      "9021/9021 [==============================] - 228s 25ms/step - loss: 0.2035 - accuracy: 0.9166 - val_loss: 0.3454 - val_accuracy: 0.8664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x45bf0e610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open('model5.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model weights\n",
    "model.save_weights('model_weights5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('lstm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256/2256 [==============================] - 15s 6ms/step\n",
      "2256/2256 [==============================] - 14s 6ms/step\n",
      "Accuracy: 0.8389152936937936\n",
      "Precision: 0.8017638036809815\n",
      "Recall: 0.7948097722796227\n",
      "F1 Score: 0.7982716435004424\n",
      "AUC-ROC: 0.9027370638487501\n",
      "Confusion Matrix:\n",
      "[[37541  5687]\n",
      " [ 5938 23001]]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict(X_test_padded))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_df = pd.read_csv('data/undersampled_data_60_40_ft.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'comment_text', 'toxic', 'stopwords_punct_lemma',\n",
       "       'toxic_label_ft', 'toxic_label_comment_text', 'vector_fast_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.0577833019, 0.0458838157, -0.0487854704, -...\n",
       "1    [-0.0385174714, 0.0294841994, -0.0353648514, -...\n",
       "2    [0.08621803, -0.06944817, 0.08360571, 0.003052...\n",
       "3    [-0.02172438, 0.01810819, -0.02264511, -0.0008...\n",
       "4    [-0.04083619, 0.03226621, -0.03952266, -0.0020...\n",
       "Name: new_ft, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_df['new_ft'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vect = ft_df['vector_fast_text'].str.strip('[]').str.split(expand=True)\n",
    "corpus_vect = corpus_vect.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_df['new_ft'] = corpus_vect.values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC + fast_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9696090811950986\n"
     ]
    }
   ],
   "source": [
    "X = np.array(ft_df['new_ft'].tolist())\n",
    "y = ft_df['toxic']\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Flatten X_train and X_test\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "rfc.fit(X_train_flatten, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rfc.predict(X_test_flatten)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9696090811950986\n",
      "Precision: 0.9666179337231969\n",
      "Recall: 0.957551724137931\n",
      "F1 Score: 0.9620634700665189\n",
      "AUC-ROC: 0.9888729088406262\n",
      "Confusion Matrix:\n",
      "[[42102   959]\n",
      " [ 1231 27769]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, rfc.predict_proba(X_test_flatten)[:, 1])\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee5ce8695951d3743acb1574d7cbc518a435066b16546d59d44a9748127e061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
