{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChnD36qHhuy"
      },
      "source": [
        "# Data cleaning FINAL (Michael)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTUtsD6HupB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisbn3mTG0fj",
        "outputId": "c3a65a47-0bea-4de2-c594-1b5ca57dec48"
      },
      "outputs": [],
      "source": [
        "# import the usual suspects / basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# tqdm\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# spaCy\n",
        "import spacy\n",
        "#!python -m spacy download en_core_web_sm # must be run just once\n",
        "\n",
        "# fastText\n",
        "import fasttext\n",
        "\n",
        "# display all df columns (default is 20)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# show all data in columns so that full comment is visible\n",
        "pd.options.display.max_colwidth = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MP847vfIJMN"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6YNY0NIIL4d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/undersampled_data_60_40_ft.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Create smaller sample from data to speed up things while experimenting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_size = None\n",
        "\n",
        "# uncomment to create sample of desired size\n",
        "#sample_size = 10_000\n",
        "\n",
        "if sample_size != None:\n",
        "    # ratio toxic/nontoxic\n",
        "    tox_perc = 0.4\n",
        "    nontox_perc = 0.6\n",
        "\n",
        "    # number of toxic/nontoxic rows\n",
        "    sample_size_tox = int(sample_size * tox_perc)\n",
        "    sample_size_nontox = int(sample_size * nontox_perc)\n",
        "\n",
        "    sample_tox = df[df['toxic'] == 1].sample(sample_size_tox,\n",
        "                                             random_state=42)\n",
        "    sample_nontox = df[df['toxic'] == 0].sample(sample_size_nontox,\n",
        "                                                random_state=42)\n",
        "\n",
        "    df = pd.concat([sample_tox, sample_nontox])\n",
        "    print(f'Using sample ({df.shape[0]} rows).')\n",
        "\n",
        "else:\n",
        "    print(f'Using full data ({df.shape[0]} rows).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = df['comment_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show data size before cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count 'words' (rough regex method)\n",
        "num_words_before = corp.str.count(r'\\S+', flags=re.I).sum()\n",
        "\n",
        "print(f'Number of words in corpus before cleaning: {num_words_before:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove anchor HTML tags (\\<a\\>)\n",
        "\n",
        "TODO: Do this with an HTML parser like Beautiful Soup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex = r'<a .*?>|</a>' # *? for non-greedy repetition\n",
        "\n",
        "# count matches\n",
        "print(corp.str.count(regex, flags=re.I).sum())\n",
        "\n",
        "# show some rows containing the pattern\n",
        "corp[corp.str.contains(regex, na=False, case=False)].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# replace pattern\n",
        "corp = corp.str.replace(regex, '', regex=True, case=False)\n",
        "\n",
        "# count matches again, should be 0\n",
        "print(corp.str.count(regex, flags=re.I).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex = r'https?://\\S+'\n",
        "print(corp.str.count(regex, flags=re.I).sum())\n",
        "corp[corp.str.contains(regex, na=False, case=False)].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = corp.str.replace(regex, '', regex=True, case=False)\n",
        "print(corp.str.count(regex, flags=re.I).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove whitespace except for spaces\n",
        "\n",
        "\\r actually causes an error when loading the saved csv file with read_csv() (just C engine, Python engine works).  \n",
        "\\u2028 --> Unicode line seperator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex = r'[\\t\\n\\r\\f\\v\\u2028]'\n",
        "print(corp.str.count(regex, flags=re.I).sum())\n",
        "corp[corp.str.contains(regex, na=False, case=False)].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = corp.str.replace(regex, ' ', regex=True, case=False)\n",
        "print(corp.str.count(regex, flags=re.I).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex = r'\\d+'\n",
        "print(corp.str.count(regex, flags=re.I).sum())\n",
        "corp[corp.str.contains(regex, na=False, case=False)].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = corp.str.replace(regex, ' ', regex=True, case=False)\n",
        "print(corp.str.count(regex, flags=re.I).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manually \"unmask\" morst frequent swearwords, insults etc. (e.g. f*ck, cr@p)\n",
        "\n",
        "Also correct some (on-purpose) misspellings that reflect pronunciation, e.g. \"huuuge\", \"stooopid\".\n",
        "\n",
        "TODO: Implement autocorrection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# search patterns used to create list of replacements (see next cell)\n",
        "\n",
        "regex = r'\\S*\\*\\S+'\n",
        "#regex = r'\\S*@\\S+'\n",
        "#regex = r'\\S*#\\S+'\n",
        "#regex = r'\\S*a{3,}\\S*'\n",
        "#regex = r'\\S*e{3,}\\S*'\n",
        "#regex = r'\\S*i{3,}\\S*'\n",
        "#regex = r'\\S*o{3,}\\S*'\n",
        "#regex = r'\\S*u{3,}\\S*'\n",
        "\n",
        "print(corp.str.count(regex, flags=re.I).sum())\n",
        "all_matches = corp.str.findall(regex, flags=re.I).value_counts()\n",
        "all_matches[all_matches > 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "match_list = '(?i)f*ck, (?i)sh*t, (?i)s**t, (?i)f***, (?i)p***y, (?i)b*tch, (?i)f**k, (?i)p*ssy, (?i)p****, (?i)s***, (?i)a**, (?i)h*ll, (?i)h***, (?i)sh*t, (?i)pu**y, (?i)sh**, (?i)cr*p, (?i)@ss, (?i)cr@p, (?i)b@lls, (?i)f@ck, (?i)waaay, (?i)waaaay, (?i)riiiight, (?i)soo+, (?i)stooooopid, (?i)huu+ge, (?i)yuu+ge, (?i)suu+re'\\\n",
        "    .replace('*', r'\\*').split(', ')\n",
        "replace_list = 'fuck, shit, shit, fuck, pussy, bitch, fuck, pussy, pussy, shit, ass, hell, hell, shit, pussy, shit, crap, ass, crap, balls, fuck, way, way, right, so, stupid, huge, huge, sure'\\\n",
        "    .split(', ')\n",
        "\n",
        "corp.replace(match_list, replace_list, regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove multiple spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex = r' {2,}'\n",
        "print(corp.str.count(regex, flags=re.I).sum())\n",
        "corp[corp.str.contains(regex, na=False, case=False)].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp = corp.str.replace(regex, ' ', regex=True, case=False)\n",
        "print(corp.str.count(regex, flags=re.I).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show data size after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_words_after = corp.str.count(r'\\S+', flags=re.I).sum()\n",
        "\n",
        "print(f'Number of words in corpus after cleaning: {num_words_after:,} (before: {num_words_before:,})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess data with spaCy (based on Eric's pipeline)\n",
        "\n",
        "See: https://realpython.com/natural-language-processing-spacy-python/\n",
        "\n",
        "TODO: Check if NLTK is faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load English language model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize, remove punctuation, make lower case, lemmatize, remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(s):\n",
        "    doc = nlp(s)\n",
        "    \n",
        "    tokens = [token.text.lower()\n",
        "              for token in doc\n",
        "              if not token.is_punct]\n",
        "    \n",
        "    tokens_lemma = [token.lemma_.lower()\n",
        "              for token in doc\n",
        "              if not token.is_punct]\n",
        "    \n",
        "    tokens_lemma_stop = [token.lemma_.lower()\n",
        "              for token in doc\n",
        "              if not token.is_punct and not token.is_stop]\n",
        "    \n",
        "    # convert lists to space-separated strings and return as Series\n",
        "    return pd.Series([' '.join(tokens),\n",
        "                      ' '.join(tokens_lemma),\n",
        "                      ' '.join(tokens_lemma_stop)],\n",
        "                      index=['clean_pp',\n",
        "                             'clean_pp_lemma',\n",
        "                             'clean_pp_lemma_stop'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp_pp = corp.progress_apply(preprocess)\n",
        "corp_pp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create new df with raw + cleaned + preprocessed comments + target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_new = pd.concat([df['comment_text'],\n",
        "                    corp,\n",
        "                    corp_pp['clean_pp'],\n",
        "                    corp_pp['clean_pp_lemma'],\n",
        "                    corp_pp['clean_pp_lemma_stop'],\n",
        "                    df['toxic']], axis=1)\n",
        "\n",
        "# column names\n",
        "df_new.columns = ['raw',\n",
        "                  'clean',\n",
        "                  'clean_pp',\n",
        "                  'clean_pp_lemma',\n",
        "                  'clean_pp_lemma_stop',\n",
        "                  'toxic']\n",
        "\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop rows with NaN's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert empty strings to NaN\n",
        "df_new.replace('', np.NaN, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_new.isna().sum()\n",
        "rows_before = df_new.shape[0]\n",
        "print(\"Rows before dropping:\", rows_before)\n",
        "df_new.dropna(inplace=True)\n",
        "df_new.reset_index(drop=True, inplace=True)\n",
        "rows_after = df_new.shape[0]\n",
        "print('Rows after dropping:', rows_after)\n",
        "print('Rows dropped:', rows_before - rows_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create fastText vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # create temp file for fastText\n",
        "# df_new.comment_clean_preproc.to_csv('data/fasttext_training_data_tmp.csv',\n",
        "#                                     index=False, header=False)\n",
        "\n",
        "# # run unsupervised learning to get embeddings\n",
        "# ft = fasttext.train_unsupervised('data/fasttext_training_data_tmp.csv')\n",
        "\n",
        "# # delete temp file\n",
        "# os.remove('data/fasttext_training_data_tmp.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # add fastText vectors to df\n",
        "# df_new['ft_vector'] = df_new['comment_clean_preproc']\\\n",
        "#     .map(ft.get_sentence_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_new.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_new.to_csv('data/data_usampl_60_40_FINAL_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_check = pd.read_csv('data/data_usampl_60_40_FINAL_test.csv')\n",
        "df_check.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_check.isna().sum()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
