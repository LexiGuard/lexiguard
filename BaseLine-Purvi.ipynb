{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellow;\"><b>Baseline on All Data</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>Imports</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>Data from CSV</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['toxic'] = (data['toxicity'] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>Bag of Words + Logistic Regression</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using comment_text and toxic\n",
    "\n",
    "X = data['comment_text'].fillna('') \n",
    "y = data['toxic']\n",
    "\n",
    "# split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to BoW\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "log_reg_bow = LogisticRegression(random_state=42,max_iter=1000) \n",
    "log_reg_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# prediction\n",
    "\n",
    "y_pred_bow = log_reg_bow.predict(X_test_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_bow)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and logistic regression summary:\n",
    "\n",
    "<span style=\"color: grey;\">\n",
    "\n",
    "Accuracy: 94.42% - Overall correct prediction rate of 94.42%.\n",
    "\n",
    "Among the identified non-toxic comments, 96% were correct.\n",
    "\n",
    "Identified 98% of the actual non-toxic comments.\n",
    "\n",
    "Balanced measure considering precision and recall for non-toxic comments 97%.\n",
    "\n",
    "Among the identified toxic comments, 73% were correct.\n",
    "\n",
    "Identified only 48% of the actual toxic comments.\n",
    "\n",
    "Balanced measure considering precision and recall for toxic comments, resulting in 58%.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>Confusion Matrix</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f'Confusion matrix: {conf_matrix}')\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix BoW')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Explaination:\n",
    "\n",
    "<span style=\"color: grey;\">\n",
    "\n",
    "TP: 15,567. toxic comments.\n",
    "\n",
    "TN: 362,010. non-toxic comments. The model is good at identifying non-toxic comments i.e. 362,010.\n",
    "\n",
    "FP: 5,622. predicted as toxic, but actually non-toxic.\n",
    "\n",
    "FN: 16,705. predicted as non-toxic, but actually toxic.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>TF-IGF + Logistic Regression</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using comment_text and toxic\n",
    "\n",
    "X1 = data['comment_text'].fillna('') \n",
    "y1 = data['toxic']\n",
    "\n",
    "# split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to TF-IGF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X1_train_tfidf = tfidf_vectorizer.fit_transform(X1_train)\n",
    "X1_test_tfidf = tfidf_vectorizer.transform(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X1_train_tfidf, y1_train)\n",
    "\n",
    "# prediction\n",
    "y1_pred = log_reg.predict(X1_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1 = accuracy_score(y1_test, y1_pred)\n",
    "print(f\"Accuracy TF-IGF: {accuracy1:.4f}\")\n",
    "print(classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IGF and logistic regression summary:\n",
    "\n",
    "<span style=\"color: grey;\">\n",
    "\n",
    "Accuracy: 94.42% - Overall correct prediction rate of 94.72%.\n",
    "\n",
    "Among the identified non-toxic comments, 95% were correct.\n",
    "\n",
    "Identified 99% of the actual non-toxic comments.\n",
    "\n",
    "Balanced measure considering precision and recall for non-toxic comments 97%.\n",
    "\n",
    "Among the identified toxic comments, 79% were correct.\n",
    "\n",
    "Identified only 47% of the actual toxic comments.\n",
    "\n",
    "Balanced measure considering precision and recall for toxic comments, resulting in 59%.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>Confusion Matrix</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix1 = confusion_matrix(y1_test, y1_pred)\n",
    "print(f'Confusion matrix: {conf_matrix1}')\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix TF-IGF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Explaination:\n",
    "\n",
    "<span style=\"color: grey;\">\n",
    "\n",
    "TP: 15,029. toxic comments.\n",
    "\n",
    "TN: 363,744,010. non-toxic comments. The model is good at identifying non-toxic comments i.e. 362,010.\n",
    "\n",
    "FP: 3,888. predicted as toxic, but actually non-toxic.\n",
    "\n",
    "FN: 17,243. predicted as non-toxic, but actually toxic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: yellow;\"><b>Dummy Classifier</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data['comment_text'].fillna('') \n",
    "y2 = data['toxic']\n",
    "\n",
    "# split\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\")  # You can change strategy if needed\n",
    "dummy_clf.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = dummy_clf.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2 = accuracy_score(y2_test, y2_pred)\n",
    "print(f\"Accuracy Dummy Classifier: {accuracy1:.4f}\")\n",
    "print(classification_report(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"><b>Confusion Matrix: Dummy Classifier</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_dcl = confusion_matrix(y2_test, y2_pred)\n",
    "print(f'Confusion matrix: {conf_matrix_dcl}')\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_dcl, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Dummy Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Explaination:\n",
    "\n",
    "<span style=\"color: grey;\">\n",
    "\n",
    "TP: 2,538. toxic comments.\n",
    "\n",
    "TN: 338,291. non-toxic comments. The model is good at identifying non-toxic comments i.e. 362,010.\n",
    "\n",
    "FP: 29,341. predicted as toxic, but actually non-toxic.\n",
    "\n",
    "FN: 29,734. predicted as non-toxic, but actually toxic.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee5ce8695951d3743acb1574d7cbc518a435066b16546d59d44a9748127e061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
