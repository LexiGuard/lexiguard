{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and different pre-process techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the aim is to try different pre-process techniques and NLP models with Logistic regression classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericmartinez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericmartinez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "import spacy # (object oriented)\n",
    "import nltk # natural language tool kit (string oriented)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Run this lines only once\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_lg #587.7 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/merged_data.csv', nrows=1000) #nrows only to get the fist 500 rows in the data\n",
    "df_train = data[['comment_text','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis alternative data is to go with a bigger data set\\ndata = pd.read_csv('data/train.csv')\\ndf_cleaned = data.dropna(subset=['comment_text'])\\ndf_train = df_cleaned[['comment_text','target']]\\n# Add new column toxic, toxicity >= 0.5 then toxic = 1 otherwise toxic = 0\\ndf_train = df_train.copy()\\ndf_train['toxic'] = np.where(df_train['target'] >= 0.50, 1, 0)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This alternative data is to go with a bigger data set\n",
    "data = pd.read_csv('data/train.csv')\n",
    "df_cleaned = data.dropna(subset=['comment_text'])\n",
    "df_train = df_cleaned[['comment_text','target']]\n",
    "# Add new column toxic, toxicity >= 0.5 then toxic = 1 otherwise toxic = 0\n",
    "df_train = df_train.copy()\n",
    "df_train['toxic'] = np.where(df_train['target'] >= 0.50, 1, 0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['comment_text'], df_train['toxic'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to record different models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe that will include the results\n",
    "results_table = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train,y_train,X_test,y_test,results_df,model_name=\"\", parameters='', comments=''):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    predict_probab = model.predict_proba(X_test)[:,1]\n",
    "    duration = time.time() - start_time\n",
    "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
    "\n",
    "    # Calculating all metrics\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Create a dictionary including the results\n",
    "    results = {\n",
    "        'Name': model_name if model_name else model.__class__.__name__,\n",
    "        'Parameters': parameters,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Training Time': duration_format,\n",
    "        'Comments': comments\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    new_row_df = pd.DataFrame([results])\n",
    "    # don't forget to append the result to the results dataframe\n",
    "    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Baseline\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words(Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer(binary=True).fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"binary\", comments=\"Bag of words - Binary\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (Binary + Stop Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))                       \n",
    "# stop_words contains a list of 179 words that we want to remove from our comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer(binary=True, stop_words=list(stop_words)).fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"binary,stopwords\", comments=\"Bag of words - Binary/StopWords\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer with min_df\n",
    "tfidf_vect = TfidfVectorizer(min_df=30)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_tfidf, y_train, X_test_tfidf, y_test,results_table, parameters=\"min_df=30\", comments=\"TfidfVectorizer\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(Bag of words) + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test, results_table, parameters=\"\", comments=\"Stemming_cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(Bag of words(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing stemmer and countvectorizer with Stop Words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"stopwords\", comments=\"Stemming_cv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with TF - IDF and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "                         \n",
    "# stop_words contains a list of 179 words that we want to remove from our comments\n",
    "\n",
    "# Initializing stemmer and countvectorizer with Stop Words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "tfidf_analyzer = TfidfVectorizer(min_df=30, stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"min_df=30, stopwords\", comments=\"Stemming_tfidf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"\", comments=\"lemmatization_cv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = TfidfVectorizer(min_df=30).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"min_df=30\", comments=\"lemmatization_tfidf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialization\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"stopwords\", comments=\"lemmatization_cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors - Spacy library - Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialize a pre-trained model (the small version) that uses Neural Networks to build word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# convert words into vectors and Prepare X_train for the function\n",
    "docs = [nlp(text) for text in X_train]\n",
    "X_train_word_vectors = [x.vector for x in docs]\n",
    "\n",
    "# Prepare X_test for the function\n",
    "docs_test = [nlp(text) for text in X_test]\n",
    "X_test_word_vectors = [x.vector for x in docs_test]\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors - Spacy library - Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialize a pre-trained model (the medium version) that uses Neural Networks to build word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# convert words into vectors and Prepare X_train for the function\n",
    "docs = [nlp(text) for text in X_train]\n",
    "X_train_word_vectors = [x.vector for x in docs]\n",
    "\n",
    "# Prepare X_test for the function\n",
    "docs_test = [nlp(text) for text in X_test]\n",
    "X_test_word_vectors = [x.vector for x in docs_test]\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors - Spacy library - Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialize a pre-trained model (the large version) that uses Neural Networks to build word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# convert words into vectors and Prepare X_train for the function\n",
    "docs = [nlp(text) for text in X_train]\n",
    "X_train_word_vectors = [x.vector for x in docs]\n",
    "\n",
    "# Prepare X_test for the function\n",
    "docs_test = [nlp(text) for text in X_test]\n",
    "X_test_word_vectors = [x.vector for x in docs_test]\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC-ROC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.825243</td>\n",
       "      <td>0.507748</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.867347</td>\n",
       "      <td>0.712</td>\n",
       "      <td>[[  8  46]\\n [ 26 170]]</td>\n",
       "      <td>0 minutes and 0.04 seconds</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>binary</td>\n",
       "      <td>0.856459</td>\n",
       "      <td>0.558485</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.760</td>\n",
       "      <td>[[ 11  43]\\n [ 17 179]]</td>\n",
       "      <td>0 minutes and 0.02 seconds</td>\n",
       "      <td>Bag of words - Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>binary,stopwords</td>\n",
       "      <td>0.863208</td>\n",
       "      <td>0.550170</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.933673</td>\n",
       "      <td>0.768</td>\n",
       "      <td>[[  9  45]\\n [ 13 183]]</td>\n",
       "      <td>0 minutes and 0.02 seconds</td>\n",
       "      <td>Bag of words - Binary/StopWords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>min_df=30</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792</td>\n",
       "      <td>[[  2  52]\\n [  0 196]]</td>\n",
       "      <td>0 minutes and 0.0 seconds</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.513511</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>0.700</td>\n",
       "      <td>[[ 10  44]\\n [ 31 165]]</td>\n",
       "      <td>0 minutes and 0.03 seconds</td>\n",
       "      <td>Stemming_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>0.864734</td>\n",
       "      <td>0.595522</td>\n",
       "      <td>0.821101</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.776</td>\n",
       "      <td>[[ 15  39]\\n [ 17 179]]</td>\n",
       "      <td>0 minutes and 0.01 seconds</td>\n",
       "      <td>Stemming_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>min_df=30, stopwords</td>\n",
       "      <td>0.864734</td>\n",
       "      <td>0.595522</td>\n",
       "      <td>0.821101</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.776</td>\n",
       "      <td>[[ 15  39]\\n [ 17 179]]</td>\n",
       "      <td>0 minutes and 0.01 seconds</td>\n",
       "      <td>Stemming_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.820639</td>\n",
       "      <td>0.518613</td>\n",
       "      <td>0.791469</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.708</td>\n",
       "      <td>[[ 10  44]\\n [ 29 167]]</td>\n",
       "      <td>0 minutes and 0.03 seconds</td>\n",
       "      <td>lemmatization_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>min_df=30</td>\n",
       "      <td>0.820639</td>\n",
       "      <td>0.518613</td>\n",
       "      <td>0.791469</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.708</td>\n",
       "      <td>[[ 10  44]\\n [ 29 167]]</td>\n",
       "      <td>0 minutes and 0.03 seconds</td>\n",
       "      <td>lemmatization_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>0.854415</td>\n",
       "      <td>0.549225</td>\n",
       "      <td>0.802691</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.756</td>\n",
       "      <td>[[ 10  44]\\n [ 17 179]]</td>\n",
       "      <td>0 minutes and 0.01 seconds</td>\n",
       "      <td>lemmatization_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.846512</td>\n",
       "      <td>0.482804</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.736</td>\n",
       "      <td>[[  2  52]\\n [ 14 182]]</td>\n",
       "      <td>0 minutes and 0.02 seconds</td>\n",
       "      <td>word_vectors_spacy_sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.776903</td>\n",
       "      <td>0.534958</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.660</td>\n",
       "      <td>[[ 17  37]\\n [ 48 148]]</td>\n",
       "      <td>0 minutes and 0.7 seconds</td>\n",
       "      <td>word_vectors_spacy_md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.751958</td>\n",
       "      <td>0.469199</td>\n",
       "      <td>0.770053</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.620</td>\n",
       "      <td>[[ 11  43]\\n [ 52 144]]</td>\n",
       "      <td>0 minutes and 0.65 seconds</td>\n",
       "      <td>word_vectors_spacy_lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name            Parameters  F1-Score   AUC-ROC  Precision  \\\n",
       "0   LogisticRegression                        0.825243  0.507748   0.787037   \n",
       "1   LogisticRegression                binary  0.856459  0.558485   0.806306   \n",
       "2   LogisticRegression      binary,stopwords  0.863208  0.550170   0.802632   \n",
       "3   LogisticRegression             min_df=30  0.882883  0.518519   0.790323   \n",
       "4   LogisticRegression                        0.814815  0.513511   0.789474   \n",
       "5   LogisticRegression             stopwords  0.864734  0.595522   0.821101   \n",
       "6   LogisticRegression  min_df=30, stopwords  0.864734  0.595522   0.821101   \n",
       "7   LogisticRegression                        0.820639  0.518613   0.791469   \n",
       "8   LogisticRegression             min_df=30  0.820639  0.518613   0.791469   \n",
       "9   LogisticRegression             stopwords  0.854415  0.549225   0.802691   \n",
       "10  LogisticRegression                        0.846512  0.482804   0.777778   \n",
       "11  LogisticRegression                        0.776903  0.534958   0.800000   \n",
       "12  LogisticRegression                        0.751958  0.469199   0.770053   \n",
       "\n",
       "      Recall  Accuracy         Confusion Matrix               Training Time  \\\n",
       "0   0.867347     0.712  [[  8  46]\\n [ 26 170]]  0 minutes and 0.04 seconds   \n",
       "1   0.913265     0.760  [[ 11  43]\\n [ 17 179]]  0 minutes and 0.02 seconds   \n",
       "2   0.933673     0.768  [[  9  45]\\n [ 13 183]]  0 minutes and 0.02 seconds   \n",
       "3   1.000000     0.792  [[  2  52]\\n [  0 196]]   0 minutes and 0.0 seconds   \n",
       "4   0.841837     0.700  [[ 10  44]\\n [ 31 165]]  0 minutes and 0.03 seconds   \n",
       "5   0.913265     0.776  [[ 15  39]\\n [ 17 179]]  0 minutes and 0.01 seconds   \n",
       "6   0.913265     0.776  [[ 15  39]\\n [ 17 179]]  0 minutes and 0.01 seconds   \n",
       "7   0.852041     0.708  [[ 10  44]\\n [ 29 167]]  0 minutes and 0.03 seconds   \n",
       "8   0.852041     0.708  [[ 10  44]\\n [ 29 167]]  0 minutes and 0.03 seconds   \n",
       "9   0.913265     0.756  [[ 10  44]\\n [ 17 179]]  0 minutes and 0.01 seconds   \n",
       "10  0.928571     0.736  [[  2  52]\\n [ 14 182]]  0 minutes and 0.02 seconds   \n",
       "11  0.755102     0.660  [[ 17  37]\\n [ 48 148]]   0 minutes and 0.7 seconds   \n",
       "12  0.734694     0.620  [[ 11  43]\\n [ 52 144]]  0 minutes and 0.65 seconds   \n",
       "\n",
       "                           Comments  \n",
       "0                          Baseline  \n",
       "1             Bag of words - Binary  \n",
       "2   Bag of words - Binary/StopWords  \n",
       "3                   TfidfVectorizer  \n",
       "4                       Stemming_cv  \n",
       "5                       Stemming_cv  \n",
       "6                    Stemming_tfidf  \n",
       "7                  lemmatization_cv  \n",
       "8               lemmatization_tfidf  \n",
       "9                  lemmatization_cv  \n",
       "10            word_vectors_spacy_sm  \n",
       "11            word_vectors_spacy_md  \n",
       "12            word_vectors_spacy_lg  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
