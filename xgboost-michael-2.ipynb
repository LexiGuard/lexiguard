{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChnD36qHhuy"
      },
      "source": [
        "# XGBoost experiments (Michael)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTUtsD6HupB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisbn3mTG0fj",
        "outputId": "c3a65a47-0bea-4de2-c594-1b5ca57dec48"
      },
      "outputs": [],
      "source": [
        "# import the usual suspects / basics\n",
        "import time; full_run_time_start = time.time() # start timing exec right away\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from scipy import sparse\n",
        "import re\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, f1_score,\\\n",
        "    accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# currently not used and thus commented out\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "# display all df columns (default is 20)\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility function for testing models and tracking results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# empty df for storing results\n",
        "test_results = pd.DataFrame(columns=['model_name',\n",
        "                                'model_params',\n",
        "                                'data_desc',\n",
        "                                'data_size',\n",
        "                                'features_no',\n",
        "                                'f1',\n",
        "                                'acc',\n",
        "                                'recall',\n",
        "                                'prec',\n",
        "                                'roc_auc',\n",
        "                                'cf_matrix',\n",
        "                                'train_time',\n",
        "                                'notes'])\n",
        "\n",
        "def test_model(model, model_name, model_params, data_desc, X, y, notes=''):\n",
        "    '''\n",
        "    test_model(model, model_params, data_desc, X, y, notes='')\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: instance of model to test\n",
        "    model_name: name of model\n",
        "    model_params: dict of (hyper)parameters passed to model\n",
        "    data_desc: description of dataset (preprocessing steps etc.)\n",
        "    X: feature array \n",
        "    y: target/label array\n",
        "    notes: additional notes (default: empty string)\n",
        "    '''\n",
        "\n",
        "    # Split data using default of 75% for train, 25% for test.\n",
        "    # Make sure test data has same toxic/nontoxic ratio as train data by\n",
        "    # using stratify parameter.\n",
        "    X_train, X_test, y_train, y_test =\\\n",
        "        train_test_split(X, y, stratify=y, random_state=42)\n",
        "    \n",
        "    # train model and time execution\n",
        "    train_time_start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - train_time_start\n",
        "    train_time_str = f'{int(train_time // 60)}m {round(train_time % 60)}s'\n",
        "\n",
        "    # Make predictions on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    return {'model_name': model_name,\n",
        "            'model_params': model_params,\n",
        "            'data_desc': data_desc,\n",
        "            'data_size': X.shape[0],\n",
        "            'features_no': X.shape[1],\n",
        "            'f1': round(f1_score(y_test, y_pred), 3),\n",
        "            'acc': round(accuracy_score(y_test, y_pred), 3),\n",
        "            'recall': round(recall_score(y_test, y_pred), 3),\n",
        "            'prec': round(precision_score(y_test, y_pred), 3),\n",
        "            'roc_auc': round(roc_auc_score(y_test, y_pred_proba), 3),\n",
        "            'cf_matrix': confusion_matrix(y_test, y_pred),\n",
        "            'train_time': train_time_str,\n",
        "            'notes': notes}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_test_result(result):\n",
        "    test_results.loc[len(test_results)] = result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MP847vfIJMN"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r6YNY0NIIL4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(360301, 7)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('data/undersampled_data_60_40_ft.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Create smaller sample from data to speed up experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_size = None\n",
        "\n",
        "# uncomment to create sample of desired size\n",
        "#sample_size = 25_000\n",
        "\n",
        "if sample_size != None:\n",
        "    # ratio toxic/nontoxic\n",
        "    tox_perc = 0.4\n",
        "    nontox_perc = 0.6\n",
        "\n",
        "    # number of toxic/nontoxic rows\n",
        "    sample_size_tox = int(sample_size * tox_perc)\n",
        "    sample_size_nontox = int(sample_size * nontox_perc)\n",
        "\n",
        "    sample_tox = df[df['toxic'] == 1].sample(sample_size_tox,\n",
        "                                             random_state=42)\n",
        "    sample_nontox = df[df['toxic'] == 0].sample(sample_size_nontox,\n",
        "                                                random_state=42)\n",
        "\n",
        "    df = pd.concat([sample_tox, sample_nontox])\n",
        "    print(f'Using sample ({df.shape[0]} rows).')\n",
        "\n",
        "else:\n",
        "    print(f'Using full data ({df.shape[0]} rows).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop rows with NaN's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rows with NaN's before dropping: 360273\n",
            "rows after: 360273\n",
            "rows dropped: 0\n"
          ]
        }
      ],
      "source": [
        "rows_before = df.shape[0]\n",
        "print(\"rows with NaN's before dropping:\", df.shape[0])\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print('rows after:', df.shape[0])\n",
        "print('rows dropped:', rows_before - df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IbUc5bP0IUIS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 360273 entries, 0 to 360272\n",
            "Data columns (total 7 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   Unnamed: 0                360273 non-null  int64 \n",
            " 1   comment_text              360273 non-null  object\n",
            " 2   toxic                     360273 non-null  int64 \n",
            " 3   stopwords_punct_lemma     360273 non-null  object\n",
            " 4   toxic_label_ft            360273 non-null  object\n",
            " 5   toxic_label_comment_text  360273 non-null  object\n",
            " 6   vector_fast_text          360273 non-null  object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 19.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lZffM2npRCPf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>stopwords_punct_lemma</th>\n",
              "      <th>toxic_label_ft</th>\n",
              "      <th>toxic_label_comment_text</th>\n",
              "      <th>vector_fast_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Well, what are the chances he will turn out to...</td>\n",
              "      <td>0</td>\n",
              "      <td>chance turn active proponent slavery</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 chance turn active proponent slavery</td>\n",
              "      <td>[-5.77833019e-02  4.58838157e-02 -4.87854704e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The moment of critical mass is approaching whe...</td>\n",
              "      <td>0</td>\n",
              "      <td>moment critical mass approach deed gupta co li...</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 moment critical mass approach deed ...</td>\n",
              "      <td>[-3.85174714e-02  2.94841994e-02 -3.53648514e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\"Hey listen to me,\" he said. \"I'm not going to...</td>\n",
              "      <td>1</td>\n",
              "      <td>hey listen say go crap prove reporter say    u...</td>\n",
              "      <td>__label__1</td>\n",
              "      <td>__label__1 hey listen say go crap prove report...</td>\n",
              "      <td>[ 0.08621803 -0.06944817  0.08360571  0.003052...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>We are already owed $488 M plus interest($2Bil...</td>\n",
              "      <td>0</td>\n",
              "      <td>owe 488 m plus interest 2billion 2006 audits s...</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 owe 488 m plus interest 2billion 20...</td>\n",
              "      <td>[-0.02172438  0.01810819 -0.02264511 -0.000863...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>There is a reason there are no teeth to the la...</td>\n",
              "      <td>0</td>\n",
              "      <td>reason tooth law unlawful law way force free e...</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 reason tooth law unlawful law way f...</td>\n",
              "      <td>[-0.04083619  0.03226621 -0.03952266 -0.002016...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360268</th>\n",
              "      <td>360830</td>\n",
              "      <td>Do you still beat your wife? Simple question.</td>\n",
              "      <td>0</td>\n",
              "      <td>beat wife simple question</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 beat wife simple question</td>\n",
              "      <td>[-0.11675262  0.09984541 -0.11092692  0.011191...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360269</th>\n",
              "      <td>360831</td>\n",
              "      <td>The fascist dictator continues the insanity ag...</td>\n",
              "      <td>1</td>\n",
              "      <td>fascist dictator continue insanity human civil...</td>\n",
              "      <td>__label__1</td>\n",
              "      <td>__label__1 fascist dictator continue insanity ...</td>\n",
              "      <td>[ 0.03221355 -0.02136803  0.02484508 -0.001221...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360270</th>\n",
              "      <td>360832</td>\n",
              "      <td>Sean Hannity is a lightweight foolish commenta...</td>\n",
              "      <td>0</td>\n",
              "      <td>sean hannity lightweight foolish commentator f...</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 sean hannity lightweight foolish co...</td>\n",
              "      <td>[-0.02342204  0.01447476 -0.01676093 -0.005626...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360271</th>\n",
              "      <td>360833</td>\n",
              "      <td>There are a number of countries which make it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>number country impossible national citizenship...</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 number country impossible national ...</td>\n",
              "      <td>[-0.01418185  0.01380997 -0.00814179 -0.004513...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360272</th>\n",
              "      <td>360834</td>\n",
              "      <td>Where's data strategy to track % of foreign bu...</td>\n",
              "      <td>0</td>\n",
              "      <td>datum strategy track foreign buyer</td>\n",
              "      <td>__label__0</td>\n",
              "      <td>__label__0 datum strategy track foreign buyer</td>\n",
              "      <td>[-0.06213957  0.04279981 -0.04441012  0.006022...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>360273 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0                                       comment_text  toxic  \\\n",
              "0                0  Well, what are the chances he will turn out to...      0   \n",
              "1                1  The moment of critical mass is approaching whe...      0   \n",
              "2                2  \"Hey listen to me,\" he said. \"I'm not going to...      1   \n",
              "3                3  We are already owed $488 M plus interest($2Bil...      0   \n",
              "4                4  There is a reason there are no teeth to the la...      0   \n",
              "...            ...                                                ...    ...   \n",
              "360268      360830      Do you still beat your wife? Simple question.      0   \n",
              "360269      360831  The fascist dictator continues the insanity ag...      1   \n",
              "360270      360832  Sean Hannity is a lightweight foolish commenta...      0   \n",
              "360271      360833  There are a number of countries which make it ...      0   \n",
              "360272      360834  Where's data strategy to track % of foreign bu...      0   \n",
              "\n",
              "                                    stopwords_punct_lemma toxic_label_ft  \\\n",
              "0                    chance turn active proponent slavery     __label__0   \n",
              "1       moment critical mass approach deed gupta co li...     __label__0   \n",
              "2       hey listen say go crap prove reporter say    u...     __label__1   \n",
              "3       owe 488 m plus interest 2billion 2006 audits s...     __label__0   \n",
              "4       reason tooth law unlawful law way force free e...     __label__0   \n",
              "...                                                   ...            ...   \n",
              "360268                          beat wife simple question     __label__0   \n",
              "360269  fascist dictator continue insanity human civil...     __label__1   \n",
              "360270  sean hannity lightweight foolish commentator f...     __label__0   \n",
              "360271  number country impossible national citizenship...     __label__0   \n",
              "360272                 datum strategy track foreign buyer     __label__0   \n",
              "\n",
              "                                 toxic_label_comment_text  \\\n",
              "0         __label__0 chance turn active proponent slavery   \n",
              "1       __label__0 moment critical mass approach deed ...   \n",
              "2       __label__1 hey listen say go crap prove report...   \n",
              "3       __label__0 owe 488 m plus interest 2billion 20...   \n",
              "4       __label__0 reason tooth law unlawful law way f...   \n",
              "...                                                   ...   \n",
              "360268               __label__0 beat wife simple question   \n",
              "360269  __label__1 fascist dictator continue insanity ...   \n",
              "360270  __label__0 sean hannity lightweight foolish co...   \n",
              "360271  __label__0 number country impossible national ...   \n",
              "360272      __label__0 datum strategy track foreign buyer   \n",
              "\n",
              "                                         vector_fast_text  \n",
              "0       [-5.77833019e-02  4.58838157e-02 -4.87854704e-...  \n",
              "1       [-3.85174714e-02  2.94841994e-02 -3.53648514e-...  \n",
              "2       [ 0.08621803 -0.06944817  0.08360571  0.003052...  \n",
              "3       [-0.02172438  0.01810819 -0.02264511 -0.000863...  \n",
              "4       [-0.04083619  0.03226621 -0.03952266 -0.002016...  \n",
              "...                                                   ...  \n",
              "360268  [-0.11675262  0.09984541 -0.11092692  0.011191...  \n",
              "360269  [ 0.03221355 -0.02136803  0.02484508 -0.001221...  \n",
              "360270  [-0.02342204  0.01447476 -0.01676093 -0.005626...  \n",
              "360271  [-0.01418185  0.01380997 -0.00814179 -0.004513...  \n",
              "360272  [-0.06213957  0.04279981 -0.04441012  0.006022...  \n",
              "\n",
              "[360273 rows x 7 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create label/target variable and check for imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29jE5PSrPFE2"
      },
      "outputs": [],
      "source": [
        "target = df['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "value_counts = target.value_counts()\n",
        "nontoxic_count = value_counts[0]\n",
        "toxic_count = value_counts[1]\n",
        "nontoxic_perc =\\\n",
        "    round((nontoxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
        "toxic_perc =\\\n",
        "    round((toxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
        "\n",
        "print(f'Nontoxic (0): {nontoxic_count} ({nontoxic_perc} %)')\n",
        "print(f'Toxic (1): {toxic_count} ({toxic_perc} %)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create various corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp_raw = df['comment_text']\n",
        "corp_raw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-processed corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corp_pp = df['stopwords_punct_lemma']\n",
        "corp_pp.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Corpus of fastText vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If smaller sample: Convert vector string in csv file to df\n",
        "# and cast all cols as float. This takes ~50 min for the full 360,000 rows.\n",
        "# --> If full data: Load pickle file to save time.\n",
        "\n",
        "if sample_size != None:\n",
        "    corp_ft = df['vector_fast_text'].str.strip('[]').str.split(expand=True)\n",
        "    corp_ft = corp_ft.astype('float')\n",
        "    display(corp_ft)\n",
        "    # with open('pickle/ft_vectors.pkl', mode='wb') as f:\n",
        "    #     pickle.dump(corp_ft, f)\n",
        "\n",
        "else:\n",
        "    with open('pickle/ft_vectors.pkl', mode='rb') as f:\n",
        "        corp_ft = pickle.load(f)\n",
        "    display(corp_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow = CountVectorizer()\n",
        "corp_bow = vect_bow.fit_transform(corp_raw)\n",
        "corp_bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow_bin = CountVectorizer(binary=True)\n",
        "corp_bow_bin = vect_bow_bin.fit_transform(corp_raw)\n",
        "corp_bow_bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (mixed case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow_mixc = CountVectorizer(lowercase=False)\n",
        "corp_bow_mixc = vect_bow_mixc.fit_transform(corp_raw)\n",
        "corp_bow_mixc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (default) on preprocessed comments (lemmatization, stopword and punctuation removal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow = CountVectorizer()\n",
        "corp_pp_bow = vect_bow.fit_transform(corp_pp)\n",
        "corp_pp_bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of 1/2-grams (default) on preprocessed comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bo12grams = CountVectorizer(ngram_range=(1,2))\n",
        "corp_pp_bo12grams = vect_bo12grams.fit_transform(corp_pp)\n",
        "corp_pp_bo12grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of 1/2/3-grams (default) on preprocessed comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bo123grams = CountVectorizer(ngram_range=(1,3))\n",
        "corp_pp_bo123grams = vect_bo123grams.fit_transform(corp_pp)\n",
        "corp_pp_bo123grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of 2-grams (default) on preprocessed comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bo2grams = CountVectorizer(ngram_range=(2,2))\n",
        "corp_pp_bo2grams = vect_bo2grams.fit_transform(corp_pp)\n",
        "corp_pp_bo2grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_tfidf = TfidfVectorizer()\n",
        "corp_tfidf = vect_tfidf.fit_transform(corp_raw)\n",
        "corp_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tf_idf on preprocessed comments (lemmatization, stopword and punctuation removal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_tfidf = TfidfVectorizer()\n",
        "corp_pp_tfidf = vect_tfidf.fit_transform(corp_pp)\n",
        "corp_pp_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline model (logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'max_iter': 2_000}\n",
        "\n",
        "# load model with parameters\n",
        "lr = LogisticRegression(**params)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words', corp_bow, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words',\n",
        "                         corp_bow, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words (binary)',\n",
        "                         corp_bow_bin, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words (mixed case)',\n",
        "                         corp_bow_mixc, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words (preprocessed)',\n",
        "                         corp_pp_bow, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params,\n",
        "                         'bag of 1/2-grams (preprocessed)',\n",
        "                         corp_pp_bo12grams, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # parameters for model\n",
        "# params = {'random_state': 42,\n",
        "#           'n_jobs': -1}\n",
        "\n",
        "# # load model with parameters\n",
        "# xgb = XGBClassifier(**params)\n",
        "\n",
        "# test_result = test_model(xgb, 'XGBoost', params,\n",
        "#                          'bag of 1/2/3-grams (preprocessed)',\n",
        "#                          corp_pp_bo123grams, target)\n",
        "# store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params,\n",
        "                         'bag of 2-grams (preprocessed)',\n",
        "                         corp_pp_bo2grams, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'tf_idf',\n",
        "                         corp_tfidf, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'tf_idf (preprocessed)',\n",
        "                         corp_pp_tfidf, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # parameters for model\n",
        "# params = {'random_state': 42,\n",
        "#           'n_jobs': -1,\n",
        "#           'n_estimators': 1000}\n",
        "\n",
        "# # load model with parameters\n",
        "# xgb = XGBClassifier(**params)\n",
        "\n",
        "# test_result = test_model(xgb, 'XGBoost', params, 'tf_idf (preprocessed)',\n",
        "#                          corp_pp_tfidf, target)\n",
        "# store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2b5q-TtHlGq",
        "outputId": "2aa1220d-5ebf-47f0-888d-3ccbf7b1f1b2"
      },
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'fastText vectors',\n",
        "                         corp_ft, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1,\n",
        "          'n_estimators': 1000}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'fastText vectors',\n",
        "                         corp_ft, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show test results + total exec time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_run_time = time.time() - full_run_time_start\n",
        "print(f'Full run time: {int(full_run_time // 60)}m {round(full_run_time % 60)}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate average comment length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0          17\n",
              "1          45\n",
              "2          38\n",
              "3         106\n",
              "4          49\n",
              "         ... \n",
              "360268      8\n",
              "360269     28\n",
              "360270     25\n",
              "360271     40\n",
              "360272      9\n",
              "Name: comment_text, Length: 360273, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average comment length:\n",
            "290 characters\n",
            "50 words\n"
          ]
        }
      ],
      "source": [
        "# characters\n",
        "comm_len_chars = df['comment_text'].apply(lambda s: len(s))\n",
        "avg_comm_len_chars = comm_len_chars.sum() / len(comm_len_chars)\n",
        "\n",
        "# words (rough count)\n",
        "comm_len_words = df['comment_text']\\\n",
        "    .apply(lambda s: len(re.findall(r'\\S+', s)))\n",
        "avg_comm_len_words = comm_len_words.sum() / len(comm_len_words)\n",
        "\n",
        "print('Average comment length:')\n",
        "print(round(avg_comm_len_chars), 'characters')\n",
        "print(round(avg_comm_len_words), 'words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0                  0\n",
              "comment_text                0\n",
              "toxic                       0\n",
              "stopwords_punct_lemma       0\n",
              "toxic_label_ft              0\n",
              "toxic_label_comment_text    0\n",
              "vector_fast_text            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
