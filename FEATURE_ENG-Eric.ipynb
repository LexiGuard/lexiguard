{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "import spacy\n",
    "import nltk # natural language tool kit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vectors = Vectors(shape=(10000, 300))\n",
    "\n",
    "data = np.zeros((3, 300), dtype='f')\n",
    "keys = [\"cat\", \"dog\", \"rat\"]\n",
    "vectors = Vectors(data=data, keys=keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "df_cleaned = data.dropna(subset=['comment_text'])\n",
    "df_train = df_cleaned[['comment_text','target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column toxic, toxicity >= 0.5 then toxic = 1 otherwise toxic = 0\n",
    "df_train = df_train.copy()\n",
    "df_train['toxic'] = np.where(df_train['target'] >= 0.50, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['comment_text'], df_train['toxic'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to record different models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe that will include the results\n",
    "results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train,y_train,X_test,y_test, model_name=\"\", parameters='', comments=''):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    predict_probab = model.predict_proba(X_test)[:,1]\n",
    "    duration = time.time() - start_time\n",
    "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
    "\n",
    "    # Calculating all metrics\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predict_probab)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Create a dictionary including the results\n",
    "    results = {\n",
    "        'Name': model_name if model_name else model.__class__.__name__,\n",
    "        'Parameters': parameters,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Training Time': duration_format,\n",
    "        'Comments': comments\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model (Bag of Words + LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer(binary=True).fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "result = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test, parameters=\"binary\", comments=\"Baseline\" )\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_row_df = pd.DataFrame([result])\n",
    "# don't forget to append the result to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame(new_row_df)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer with min_df\n",
    "tfidf_vect = TfidfVectorizer(min_df=30)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "result = evaluate_model(model, X_train_tfidf, y_train, X_test_tfidf, y_test, parameters=\"min_df=30\", comments=\"TfidfVectorizer\" )\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_row_df = pd.DataFrame([result])\n",
    "# don't forget to append the result to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame(new_row_df)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(Bag of words) + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "result = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test, parameters=\"\", comments=\"Stemming+LogisticRegression\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_row_df = pd.DataFrame([result])\n",
    "# don't forget to append the result to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame(new_row_df)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(Bag of words(stopwords)) + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                         \n",
    "# stop_words contains a list of 179 words that we want to remove from our comments\n",
    "\n",
    "# Initializing stemmer and countvectorizer with Stop Words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "result = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test, parameters=\"stopwords\", comments=\"Stemming_cv\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_row_df = pd.DataFrame([result])\n",
    "# don't forget to append the result to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame(new_row_df)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "result = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test, parameters=\"\", comments=\"lemmatization_cv\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "new_row_df = pd.DataFrame([result])\n",
    "# don't forget to append the result to the results dataframe\n",
    "results_df = pd.concat([results_df, pd.DataFrame(new_row_df)], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors - Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialize a pre-trained model (the small version) that uses Neural Networks to build word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
