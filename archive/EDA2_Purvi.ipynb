{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyzing Subgroups**\n",
    "\n",
    "- heatmaps to explore correlations between different identity mentions and toxicity scores\n",
    "- The mean toxicity score for each subgroup and use bar charts to see the difference\n",
    "\n",
    "**Comparative Analysis**\n",
    "\n",
    "- **Compare Subgroups**: Use side-by-side comparisons to see how different subgroups stack up against each other in terms of toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellow;\">Imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellow;\">Using merge_data.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = pd.read_csv('data/merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: Orange;\"><b>Heatmaps</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\"><b>Define Subgroups</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subgroup\n",
    "gender_cols = ['male', 'female', 'bisexual', 'transgender', 'heterosexual', 'other_gender', 'homosexual_gay_or_lesbian', 'other_sexual_orientation']\n",
    "race_cols = ['asian', 'black', 'white', 'latino', 'other_race_or_ethnicity']\n",
    "religion_cols = ['hindu', 'buddhist', 'christian', 'muslim', 'jewish', 'atheist', 'other_religion']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\"><b>Correlation</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = eda[eda['toxic'] == 1] # toxic data have values where toxic = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap using toxic = 1 and columns>= 0.5\n",
    "\n",
    "t_gend = (toxic_data[gender_cols].values >= 0.5).any(axis=1)\n",
    "corr_gender = toxic_data.loc[t_gend, gender_cols + ['toxicity']]\n",
    "\n",
    "t_race = (toxic_data[race_cols].values >= 0.5).any(axis=1)\n",
    "corr_race = toxic_data.loc[t_race, race_cols + ['toxicity']]\n",
    "\n",
    "t_rel = (toxic_data[religion_cols].values >= 0.5).any(axis=1)\n",
    "corr_rel = toxic_data.loc[t_rel, religion_cols + ['toxicity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate correlations\n",
    "corr_gender_matrix = corr_gender.corr()\n",
    "corr_race_matrix = corr_race.corr()\n",
    "corr_rel_matrix = corr_rel.corr()\n",
    "\n",
    "# Create subplots for each heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot correlation heatmaps\n",
    "sns.heatmap(corr_gender_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Correlation Heatmap - Gender')\n",
    "\n",
    "sns.heatmap(corr_race_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[1])\n",
    "axes[1].set_title('Correlation Heatmap - Race')\n",
    "\n",
    "sns.heatmap(corr_rel_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[2])\n",
    "axes[2].set_title('Correlation Heatmap - Religion')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation for all columns together\n",
    "\n",
    "corr_gender_all = toxic_data.loc[t_gend, gender_cols + ['toxicity']]\n",
    "corr_race_all = toxic_data.loc[t_race, race_cols]\n",
    "corr_rel_all = toxic_data.loc[t_rel, religion_cols ]\n",
    "\n",
    "\n",
    "all_data = pd.concat([corr_gender_all, corr_race_all, corr_rel_all], axis=1)\n",
    "\n",
    "# Calculate overall correlation matrix\n",
    "corr_all_data = all_data.corr()\n",
    "\n",
    "# Plot combined correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_all_data, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 8})\n",
    "plt.title('Combined Correlation Heatmap - All Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: Orange;\"><b>Mean</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Means:\n",
    "\n",
    "#  gender columns\n",
    "mean_gender = eda[gender_cols].mean()\n",
    "\n",
    "# race columns\n",
    "mean_race = eda[race_cols].mean()\n",
    "\n",
    "# religion columns\n",
    "mean_religion = eda[religion_cols].mean()\n",
    "\n",
    "# Store mean in the list\n",
    "means_list = [mean_gender, mean_race, mean_religion]\n",
    "\n",
    "# Print means from the list\n",
    "print(\"Mean of Gender Columns:\")\n",
    "print(mean_gender)\n",
    "\n",
    "print(\"\\nMean of Race Columns:\")\n",
    "print(mean_race)\n",
    "\n",
    "print(\"\\nMean of Religion Columns:\")\n",
    "print(mean_religion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean toxicity for all subgroups for all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_gender = {\n",
    "    'male': 0.108047,\n",
    "    'female': 0.126652,\n",
    "    'bisexual': 0.001893,\n",
    "    'transgender': 0.006712,\n",
    "    'heterosexual': 0.003248,\n",
    "    'other_gender': 0.000882,\n",
    "    'homosexual_gay_or_lesbian': 0.025378,\n",
    "    'other_sexual_orientation': 0.001492\n",
    "}\n",
    "\n",
    "mean_race = {\n",
    "    'asian': 0.011886,\n",
    "    'black': 0.034276,\n",
    "    'white': 0.056535,\n",
    "    'latino': 0.006151,\n",
    "    'other_race_or_ethnicity': 0.008158\n",
    "}\n",
    "\n",
    "mean_religion = {\n",
    "    'hindu': 0.001443,\n",
    "    'buddhist': 0.001393,\n",
    "    'christian': 0.095184,\n",
    "    'muslim': 0.049078,\n",
    "    'jewish': 0.017910,\n",
    "    'atheist': 0.003468,\n",
    "    'other_religion': 0.006718\n",
    "}\n",
    "\n",
    "mean_gender_sorted1 = dict(sorted(mean_gender.items(), key=lambda item: item[1]))\n",
    "mean_race_sorted1 = dict(sorted(mean_race.items(), key=lambda item: item[1]))\n",
    "mean_religion_sorted1 = dict(sorted(mean_religion.items(), key=lambda item: item[1]))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(mean_gender_sorted1.keys(), mean_gender_sorted1.values(), label='Gender', color='#9CC8ED',hatch='/')\n",
    "plt.bar(mean_race_sorted1.keys(), mean_race_sorted1.values(), label='Race', color='#F4E8AB',hatch='o')\n",
    "plt.bar(mean_religion_sorted1.keys(), mean_religion_sorted1.values(), label='Religion', color='#AAE2C3',hatch='.')\n",
    "\n",
    "plt.xlabel('Subgroups')\n",
    "plt.ylabel('Mean Values')\n",
    "plt.title('Mean Toxicity Scores for Different Subgroups')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Adding data labels to each bar\n",
    "for data in [mean_gender_sorted1, mean_race_sorted1, mean_religion_sorted1]:\n",
    "    for subgroup, value in data.items():\n",
    "        plt.text(subgroup, value + 0.001, round(value, 3), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean toxicity for all subgroups for all the values of each column >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean columns\n",
    "\n",
    "mean_toxic_gender = toxic_data[gender_cols].apply(lambda x: x[x >= 0.5].mean())\n",
    "mean_toxic_race = toxic_data[race_cols].apply(lambda x: x[x >= 0.5].mean())\n",
    "mean_toxic_religion = toxic_data[religion_cols].apply(lambda x: x[x >= 0.5].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of Gender Columns (>= 0.5 where toxic is 1):\")\n",
    "print(mean_toxic_gender)\n",
    "\n",
    "print(\"\\nMean of Race Columns (>= 0.5 where toxic is 1):\")\n",
    "print(mean_toxic_race)\n",
    "\n",
    "print(\"\\nMean of Religion Columns (>= 0.5 where toxic is 1):\")\n",
    "print(mean_toxic_religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gender = {\n",
    "    'male': 0.832137,\n",
    "    'female': 0.890529,\n",
    "    'bisexual': 0.683539,\n",
    "    'transgender': 0.821347,\n",
    "    'heterosexual': 0.747159,\n",
    "    'other_gender': 0.533333,\n",
    "    'homosexual_gay_or_lesbian': 0.878025,\n",
    "    'other_sexual_orientation': 0.533333\n",
    "}\n",
    "\n",
    "mean_race = {\n",
    "    'asian': 0.758636,\n",
    "    'black': 0.898275,\n",
    "    'white': 0.896285,\n",
    "    'latino': 0.729187,\n",
    "    'other_race_or_ethnicity': 0.567602\n",
    "}\n",
    "\n",
    "mean_religion = {\n",
    "    'hindu': 0.762626,\n",
    "    'buddhist': 0.731650,\n",
    "    'christian': 0.864552,\n",
    "    'muslim': 0.905658,\n",
    "    'jewish': 0.896070,\n",
    "    'atheist': 0.847757,\n",
    "    'other_religion': 0.528125\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in ascending order\n",
    "\n",
    "mean_gender_sorted = dict(sorted(mean_gender.items(), key=lambda item: item[1]))\n",
    "mean_race_sorted = dict(sorted(mean_race.items(), key=lambda item: item[1]))\n",
    "mean_religion_sorted = dict(sorted(mean_religion.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(mean_gender_sorted.keys(), mean_gender_sorted.values(), label='Gender', color='#9CC8ED',hatch='/')\n",
    "plt.bar(mean_race_sorted.keys(), mean_race_sorted.values(), label='Race', color='#F4E8AB',hatch='o')\n",
    "plt.bar(mean_religion_sorted.keys(), mean_religion_sorted.values(), label='Religion', color='#AAE2C3',hatch='.')\n",
    "\n",
    "\n",
    "plt.xlabel('Subgroups')\n",
    "plt.ylabel('Mean Values')\n",
    "plt.title('Mean Toxicity Scores for Different Subgroups')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "# Adding data labels to each bar\n",
    "for data in [mean_gender, mean_race, mean_religion]:\n",
    "    for subgroup, value in data.items():\n",
    "        plt.text(subgroup, value + 0.001, round(value, 3), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mean_toxicity.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: Orange;\"><b>Most common words </b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_gender = toxic_data.loc[t_gend, gender_cols + ['comment_text']]\n",
    "toxic_race = toxic_data.loc[t_race, race_cols + ['comment_text']]\n",
    "toxic_religion = toxic_data.loc[t_rel, religion_cols + ['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def most_common_words(data):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "    all_words = ' '.join(data['comment_text']).lower()\n",
    "    \n",
    "    # patterns to remove\n",
    "    additional_exclusions = ['â€™', '...', 's',\"n't\",'get','one','would']  \n",
    "    \n",
    "    # remove punctuations\n",
    "    pattern = re.compile(r'[^\\w\\s]')\n",
    "    \n",
    "    words = word_tokenize(all_words)\n",
    "    # Remove stopwords, punctuation, and apply stemming\n",
    "    filtered_words = [\n",
    "        stemmer.stem(word)\n",
    "        for word in words\n",
    "        if word not in stop_words\n",
    "        and word not in string.punctuation\n",
    "        and word not in additional_exclusions\n",
    "        and word not in ['like']  # Exclude specific word 'like'\n",
    "        and not pattern.match(word)\n",
    "    ]\n",
    "    words_count = Counter(filtered_words)\n",
    "    return words_count.most_common(10)  # Get the 10 most common words\n",
    "\n",
    "# Get the most common stemmed words for each subgroup\n",
    "most_common_gender = most_common_words(toxic_gender)\n",
    "most_common_race = most_common_words(toxic_race)\n",
    "most_common_religion = most_common_words(toxic_religion)\n",
    "\n",
    "# Prepare data for plotting\n",
    "common_words = {\n",
    "    'Gender': most_common_gender,\n",
    "    'Race': most_common_race,\n",
    "    'Religion': most_common_religion\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = {'Gender': '#9CC8ED', 'Race': '#F4E8AB', 'Religion': '#AAE2C3'}\n",
    "\n",
    "for subgroup, common_words in common_words.items():\n",
    "    words, counts = zip(*common_words)\n",
    "    plt.barh([f'{subgroup}: {word}' for word in words], counts, label=subgroup,color=colors[subgroup])\n",
    "\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 10 Most Common Stemmed Words in Toxic Comments for Different Subgroups')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('most_common.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: words like People, Trump, White, Right are used in all three subcategories"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee5ce8695951d3743acb1574d7cbc518a435066b16546d59d44a9748127e061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
