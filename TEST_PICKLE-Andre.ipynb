{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABw20b-PQ85_"
      },
      "source": [
        "# Get data Ready!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N_rows and percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tUfyPPxGR3Zz"
      },
      "outputs": [],
      "source": [
        "# Decide how many lines you want to run and the % of it you want to use\n",
        "# Total lines in the file: 360835\n",
        "n_rows = 50\n",
        "percentage_rows = 99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKtKyxK8gnhP"
      },
      "source": [
        "## Importing and setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q54yPL0WgUle",
        "outputId": "f7bf0ac6-d1cc-4a90-d331-b4c63eb078ff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc, classification_report\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, pipeline\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('data/undersampled_data_60_40.csv', nrows=n_rows)\n",
        "# copy data\n",
        "df = data.copy()\n",
        "# Using only # % of datset\n",
        "df = df.sample(frac=percentage_rows / 100, random_state=42)\n",
        "# before train_split:\n",
        "df = df.dropna(subset=['stopwords_punct_lemma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df['comment_text'].values\n",
        "y = df['toxic'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_words = 10000\n",
        "max_len = 200\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pad sequences to a fixed length\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model.add(LSTM(units=64))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train_padded, y_train, epochs=4, batch_size=32, validation_data=(X_test_padded, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### inserting bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_bert_embeddings(sentences, tokenizer, model):\n",
        "    \"\"\" Extracts BERT embeddings for a list of sentences. \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Extract the embeddings from the last hidden layer\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        # Pooling: Mean of the token embeddings\n",
        "        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze().numpy()\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming X_train and X_test are lists of sentences\n",
        "X_train_embeddings = extract_bert_embeddings(X_train, tokenizer, bert_model)\n",
        "X_test_embeddings = extract_bert_embeddings(X_test, tokenizer, bert_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 768)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define LSTM model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Use 'sigmoid' for binary classification\u001b[39;00m\n",
            "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/keras/src/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 768)"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "input_dim = 768  # BERT-base embeddings have a dimension of 768\n",
        "lstm_units = 64  # Number of units in LSTM layer, can be adjusted\n",
        "num_classes = 2  # Adjust based on your classification task (binary or multi-class)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(lstm_units, input_shape=(input_dim,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Use 'sigmoid' for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',  # Use 'binary_crossentropy' for binary classification\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the model with BERT embeddings\n",
        "model.fit(X_train_embeddings, y_train, epochs=4, batch_size=32, validation_data=(X_test_embeddings, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL to pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "from keras.models import load_model\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "Life is not really beautiful. Apply the preprocessing function to the sample text and display the result.\n",
            "\n",
            "Preprocessed Text:\n",
            "life beautiful apply preprocessing function sample text display result\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Preprocess Function\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "           continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Life is not really beautiful. Apply the preprocessing function to the sample text and display the result.\"\n",
        "\n",
        "# Apply the preprocessing function to the sample text\n",
        "preprocessed_text = preprocess(sample_text)\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(preprocessed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_proba = predictions[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_proba = model.predict_proba(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import model_from_json\n",
        "from keras.utils import register_keras_serializable\n",
        "from keras.models import load_model\n",
        "\n",
        "# Register Sequential class for serialization\n",
        "# register_keras_serializable(\"keras.engine.Sequential\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# Load the architecture from JSON\n",
        "with open(\"model.json\", \"r\") as json_file:\n",
        "    loaded_model_json = json_file.read()\n",
        "\n",
        "lstm_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load the weights\n",
        "lstm_model.load_weights(\"model_weights.h5\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'tokenizer_config'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding layer not found in the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Extract the tokenizer configuration from the 'Embedding' layer\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Create a new Tokenizer with the loaded configuration\u001b[39;00m\n\u001b[1;32m     35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n",
            "\u001b[0;31mKeyError\u001b[0m: 'tokenizer_config'"
          ]
        }
      ],
      "source": [
        "from keras.models import model_from_json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the architecture from JSON\n",
        "with open(\"model.json\", \"r\") as json_file:\n",
        "    model_json = json_file.read()\n",
        "\n",
        "# Load the model\n",
        "loaded_model = model_from_json(model_json)\n",
        "\n",
        "# Load the weights\n",
        "loaded_model.load_weights(\"model_weights.h5\")\n",
        "\n",
        "# Find the 'Embedding' layer which contains the tokenizer configuration\n",
        "embedding_layer = None\n",
        "for layer in json.loads(model_json)[\"config\"][\"layers\"]:\n",
        "    if layer[\"class_name\"] == \"Embedding\":\n",
        "        embedding_layer = layer\n",
        "        break\n",
        "\n",
        "if embedding_layer is None:\n",
        "    raise ValueError(\"Embedding layer not found in the model.\")\n",
        "\n",
        "# Extract the tokenizer configuration from the 'Embedding' layer\n",
        "tokenizer_config = embedding_layer[\"config\"][\"tokenizer_config\"]\n",
        "\n",
        "# Create a new Tokenizer with the loaded configuration\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.__dict__.update(tokenizer_config)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Life is not really beautiful. Apply the preprocessing function to the sample text and display the result.\"\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessed_text = preprocess(sample_text)\n",
        "\n",
        "# Convert text to sequences using the loaded tokenizer\n",
        "sequences = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')  # Adjust maxlen accordingly\n",
        "\n",
        "# Make prediction\n",
        "predictions = loaded_model.predict(np.array(padded_sequences))\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "This is a sample text for preprocessing. It includes stopwords and punctuation!\n",
            "\n",
            "Preprocessed Text:\n",
            "sample text preprocessing include stopword punctuation\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Preprocess Function\n",
        "def preprocess(text):\n",
        "    # Load English language model and create nlp object from it\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Filter out stopwords, punctuation, and apply lemmatization\n",
        "    filtered_tokens = [token.lemma_ for token in doc if not (token.is_stop or token.is_punct)]\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"This is a sample text for preprocessing. It includes stopwords and punctuation!\"\n",
        "\n",
        "# Apply the preprocessing function to the sample text\n",
        "preprocessed_text = preprocess(sample_text)\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "Life is not really beautiful. Apply the preprocessing function to the sample text and display the result.\n",
            "\n",
            "Preprocessed Text:\n",
            "life beautiful apply preprocessing function sample text display result\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Preprocess Function\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "           continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Life is not really beautiful. Apply the preprocessing function to the sample text and display the result.\"\n",
        "\n",
        "# Apply the preprocessing function to the sample text\n",
        "preprocessed_text = preprocess(sample_text)\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(preprocessed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "file_path = os.path.abspath('svm_stop_tfidf.pkl')\n",
        "# Load the model\n",
        "with open(file_path, 'rb') as f:\n",
        "    model_ready = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_list_vec = tfidf_vectorizer.transform([preprocessed_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_ready.predict_proba(text_list_vec)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ABw20b-PQ85_",
        "K4WLd55IOVGy",
        "du7KTpc7S3J6",
        "cUrGFfCGUW-Z",
        "50BbC72pUqXa",
        "e8c7OAVYU1KN",
        "kc1jRLkWVIeK",
        "ZVWXB9JVVg7S",
        "gc0Gk2eEVlBj",
        "VHJB8n_CVp8s",
        "fdYtRs_5V9P9",
        "d2E-Gf00TBUz",
        "KmksXbnjW37r",
        "wQ71Rd6zW7sk",
        "FfI-2XAzW-WA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
