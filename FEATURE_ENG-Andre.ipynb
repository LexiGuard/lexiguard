{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in ./.venv/lib/python3.11/site-packages (from pytorch-transformers) (2.1.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from pytorch-transformers) (1.23.5)\n",
      "Collecting boto3 (from pytorch-transformers)\n",
      "  Downloading boto3-1.33.5-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from pytorch-transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from pytorch-transformers) (4.66.1)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.11/site-packages (from pytorch-transformers) (2023.10.3)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from pytorch-transformers) (0.1.99)\n",
      "Collecting sacremoses (from pytorch-transformers)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->pytorch-transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->pytorch-transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->pytorch-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->pytorch-transformers) (2023.10.0)\n",
      "Collecting botocore<1.34.0,>=1.33.5 (from boto3->pytorch-transformers)\n",
      "  Downloading botocore-1.33.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->pytorch-transformers)\n",
      "  Downloading s3transfer-0.8.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->pytorch-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->pytorch-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->pytorch-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->pytorch-transformers) (2023.11.17)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from sacremoses->pytorch-transformers) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.venv/lib/python3.11/site-packages (from botocore<1.34.0,>=1.33.5->boto3->pytorch-transformers) (2.8.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->pytorch-transformers)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.5->boto3->pytorch-transformers) (1.16.0)\n",
      "Downloading boto3-1.33.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.33.5-py3-none-any.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Installing collected packages: urllib3, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed boto3-1.33.5 botocore-1.33.5 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.8.2 sacremoses-0.1.1 urllib3-2.0.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import jupyterlab\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv', nrows=500)\n",
    "df_cleaned = data.dropna(subset=['comment_text'])\n",
    "df_train = df_cleaned[['comment_text','target']]\n",
    "\n",
    "# Add new column toxic, toxicity >= 0.5 then toxic = 1 otherwise toxic = 0\n",
    "df_train = df_train.copy()\n",
    "df_train['toxic'] = np.where(df_train['target'] >= 0.50, 1, 0)\n",
    "\n",
    "df_train_small = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only 5% of datset\n",
    "percentage = 5\n",
    "df_train_small = df_train.sample(frac=percentage / 100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'target', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_small['toxic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of [SEP] and [CLS] tokens\n",
    "sentences  = []\n",
    "for sentence in df_train_small['comment_text']:\n",
    "  sentence = sentence + \"[SEP] [CLS]\"\n",
    "  sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all dependencies again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import XLNetTokenizer,XLNetForSequenceClassification,XLNetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#split the data in train and test\\nX = df_train_small['comment_text']\\ny = df_train_small['toxic']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#split the data in train and test\n",
    "X = df_train_small['comment_text']\n",
    "y = df_train_small['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "- XLNet tokenizer is used to convert our text into tokens that correspond to XLNet’s vocabulary.\n",
    "- a sequence of integers identifying each input token to its index number in the XLNet tokenizer\n",
    "- Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True, remove_space=True,)\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 150, 115, 18, 8036, 28, 52, 1902, 9, 10849, 23, 3882, 3158, 4145, 11974, 23, 3158]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(ids[0])\n",
    "labels = df_train_small['toxic'].values\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We find the maximum length of our sentences so that we can pad the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "max1 = len(ids[0])\n",
    "for i in ids:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "print(max1)\n",
    "MAX_LEN = max1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "input_ids2 = pad_sequences(ids,maxlen=180,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(input_ids2,labels,test_size=0.15)\n",
    "\n",
    "print(len(input_ids2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = torch.tensor(xtrain)\n",
    "Ytrain = torch.tensor(ytrain)\n",
    "Xtest = torch.tensor(xtest)\n",
    "Ytest = torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(Xtrain,Ytrain)\n",
    "test_data = TensorDataset(Xtest,Ytest)\n",
    "loader = DataLoader(train_data,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -1: [-1, 768]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/andre/hh-ds-23-3/lexiguards/FEATURE_ENG-Andre.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andre/hh-ds-23-3/lexiguards/FEATURE_ENG-Andre.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m XLNetForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mxlnet-base-cased\u001b[39;49m\u001b[39m\"\u001b[39;49m,num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andre/hh-ds-23-3/lexiguards/FEATURE_ENG-Andre.ipynb#Y136sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/pytorch_transformers/modeling_utils.py:536\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading weights file \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    533\u001b[0m         archive_file, resolved_archive_file))\n\u001b[1;32m    535\u001b[0m \u001b[39m# Instantiate model.\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m from_tf:\n\u001b[1;32m    539\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(resolved_archive_file, map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/pytorch_transformers/modeling_xlnet.py:1110\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39msuper\u001b[39m(XLNetForSequenceClassification, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mnum_labels\n\u001b[0;32m-> 1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m XLNetModel(config)\n\u001b[1;32m   1111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msequence_summary \u001b[39m=\u001b[39m SequenceSummary(config)\n\u001b[1;32m   1112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39md_model, config\u001b[39m.\u001b[39mnum_labels)\n",
      "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/pytorch_transformers/modeling_xlnet.py:731\u001b[0m, in \u001b[0;36mXLNetModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclamp_len \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mclamp_len\n\u001b[1;32m    729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layer \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mn_layer\n\u001b[0;32m--> 731\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(config\u001b[39m.\u001b[39;49mn_token, config\u001b[39m.\u001b[39;49md_model)\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_emb \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mFloatTensor(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, config\u001b[39m.\u001b[39md_model))\n\u001b[1;32m    733\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([XLNetLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layer)])\n",
      "File \u001b[0;32m~/hh-ds-23-3/lexiguards/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq \u001b[39m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_parameters()\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -1: [-1, 768]"
     ]
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5)# We pass model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def flat_accuracy(preds,labels):  # A function to predict Accuracy\n",
    "  correct=0\n",
    "  for i in range(0,len(labels)):\n",
    "    if(preds[i]==labels[i]):\n",
    "      correct+=1\n",
    "  return (correct/len(labels))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train = 0\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  loss1 = []\n",
    "  steps = 0\n",
    "  train_loss = []\n",
    "  l = []\n",
    "  for inputs,labels1 in loader :\n",
    "    inputs.to(device)\n",
    "    labels1.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs.to(device))\n",
    "    loss = criterion(outputs[0],labels1.to(device)).to(device)\n",
    "    logits = outputs[1]\n",
    "    #ll=outp(loss)\n",
    "    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ] # our predicted \n",
    "    [l.append(z.item()) for z in labels1] # real labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss1.append(loss.item())\n",
    "    no_train += inputs.size(0)\n",
    "    steps += 1\n",
    "  print(\"Current Loss is : {} Step is : {} number of Example : {} Accuracy : {}\".format(loss.item(),epoch,no_train,flat_accuracy(train_loss,l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe that will include the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_train,y_train,X_test,y_test, model_name=\"\", parameters='', comments=''):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    predict_probab = model.predict_proba(X_test)[:,1]\n",
    "    duration = time.time() - start_time\n",
    "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
    "\n",
    "    # Calculating all metrics\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predict_probab)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Create a dictionary including the results\n",
    "    results = {\n",
    "        'Name': model_name if model_name else model.__class__.__name__,\n",
    "        'Parameters': parameters,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Training Time': duration_format,\n",
    "        'Comments': comments\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNET Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load XLNET tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased',\n",
    "                                           dolowercase=True,\n",
    "                                           remove_space=True,\n",
    "                                           bos_token='<s>',\n",
    "                                           unk_token='<unk>',\n",
    "                                           )\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/xlnet#transformers.XLNetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and test data\n",
    "X_train_tokens = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='tf', add_special_tokens=True) # tf for tensorflow\n",
    "X_test_tokens = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors='tf', add_special_tokens=True) # tf for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'BatchEncoding' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/andre/hh-ds-23-3/lexiguards/FEATURE_ENG-Andre.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andre/hh-ds-23-3/lexiguards/FEATURE_ENG-Andre.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train_tokens \u001b[39m=\u001b[39m X_train_tokens \u001b[39m+\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39m<sep>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<cls>\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andre/hh-ds-23-3/lexiguards/FEATURE_ENG-Andre.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_test_tokens \u001b[39m=\u001b[39m X_test_tokens \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m<sep>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m<cls>\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'BatchEncoding' and 'list'"
     ]
    }
   ],
   "source": [
    "X_train_tokens = X_train_tokens + ['<sep>', '<cls>']\n",
    "X_test_tokens = X_test_tokens + ['<sep>', '<cls>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load XLNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification # Transformer wrapper for PyTorch\n",
    "from transformers import TFXLNetForSequenceClassification # Transformer wrapper for TensorFlow\n",
    "\n",
    "# Load XLNET model\n",
    "model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)  # Assuming binary classification (toxic or not toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing\n",
    "train_dataset = Dataset.from_tensor_slices((X_train_tokens['input_ids'].numpy(),\n",
    "                                            X_train_tokens['attention_mask'].numpy(),\n",
    "                                            y_train.values))\n",
    "test_dataset = Dataset.from_tensor_slices((X_test_tokens['input_ids'].numpy(),\n",
    "                                           X_test_tokens['attention_mask'].numpy(),\n",
    "                                           y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the training data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train_tokens), seed=42)\n",
    "train_dataset = train_dataset.batch(16, drop_remainder=True)\n",
    "\n",
    "# Batch the test data\n",
    "test_dataset = test_dataset.batch(5, drop_remainder=False) # Set to False to get the last batch \n",
    "                                                           # if the total number of samples is not \n",
    "                                                           # divisible by the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow tensors\n",
    "train_dataset = train_dataset.map(lambda input_ids, attention_mask, labels: (tf.convert_to_tensor(input_ids),\n",
    "                                                                            tf.convert_to_tensor(attention_mask),\n",
    "                                                                            tf.convert_to_tensor(labels)))\n",
    "\n",
    "test_dataset = test_dataset.map(lambda input_ids, attention_mask, labels: (tf.convert_to_tensor(input_ids),\n",
    "                                                                          tf.convert_to_tensor(attention_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = Adam(learning_rate=2e-2)\n",
    "criterion = SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(1):  # You can adjust the number of epochs\n",
    "    for batch in train_dataset:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, training=True)\n",
    "            loss = criterion(labels, outputs.logits)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tokens['input_ids'], X_train_tokens['attention_mask'], torch.tensor(y_train.values))\n",
    "test_dataset = TensorDataset(X_test_tokens['input_ids'], X_test_tokens['attention_mask'], torch.tensor(y_test.values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(3):  # You can adjust the number of epochs\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        outputs = model(input_ids=batch[0], attention_mask=batch[1])\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "        # Evaluate performance metrics as needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is already trained and in the 'model' variable\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        outputs = model(input_ids=batch[0], attention_mask=batch[1])\n",
    "        predicted_probs = torch.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
    "        predicted_labels = (predicted_probs >= 0.5).astype(int)\n",
    "        all_predictions.extend(predicted_labels)\n",
    "        all_true_labels.extend(batch[2].cpu().numpy())\n",
    "\n",
    "# Calculate and print accuracy, F1 score, and AUC\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "f1 = f1_score(all_true_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_true_labels, predicted_probs)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_true_labels, all_predictions))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(all_true_labels, predicted_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
