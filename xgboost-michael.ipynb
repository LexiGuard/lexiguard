{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChnD36qHhuy"
      },
      "source": [
        "# XGBoost experiments (Michael)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTUtsD6HupB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisbn3mTG0fj",
        "outputId": "c3a65a47-0bea-4de2-c594-1b5ca57dec48"
      },
      "outputs": [],
      "source": [
        "# import the usual suspects / basics\n",
        "import time; full_run_time_start = time.time() # start timing right away\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from scipy import sparse\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, f1_score,\\\n",
        "    accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# increase number of displayed df columns, since data has quite many\n",
        "# (default is 20)\n",
        "pd.options.display.max_columns = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility function for testing models and tracking results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# empty df for storing results\n",
        "test_results = pd.DataFrame(columns=['model_name',\n",
        "                                'model_params',\n",
        "                                'data_desc',\n",
        "                                'train_data_size',\n",
        "                                'f1',\n",
        "                                'acc',\n",
        "                                'recall',\n",
        "                                'prec',\n",
        "                                'roc_auc',\n",
        "                                'cf_matrix',\n",
        "                                'train_time',\n",
        "                                'notes'])\n",
        "\n",
        "def test_model(model, model_name, model_params, data_desc, X, y, notes=''):\n",
        "    '''\n",
        "    test_model(model, model_params, data_desc, X, y, notes='')\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: instance of model to test\n",
        "    model_name: name of model\n",
        "    model_params: dict of (hyper)parameters passed to model\n",
        "    data_desc: description of dataset (preprocessing steps etc.)\n",
        "    X: feature array \n",
        "    y: target/label array\n",
        "    notes: additional notes (default: empty string)\n",
        "    '''\n",
        "\n",
        "    # Split data using default of 75% for train, 25% for test.\n",
        "    # Make sure test data has same toxic/nontoxic ratio as train data by\n",
        "    # using stratify parameter.\n",
        "    X_train, X_test, y_train, y_test =\\\n",
        "        train_test_split(X, y, stratify=y, random_state=42)\n",
        "    \n",
        "    # train model and time execution\n",
        "    train_time_start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - train_time_start\n",
        "    train_time_str = f'{int(train_time // 60)}m {round(train_time % 60)}s'\n",
        "\n",
        "    # Make predictions on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    return {'model_name': model_name,\n",
        "            'model_params': model_params,\n",
        "            'data_desc': data_desc,\n",
        "            'train_data_size': X_train.shape[0],\n",
        "            'features_no': X_train.shape[1],\n",
        "            'f1': round(f1_score(y_test, y_pred), 3),\n",
        "            'acc': round(accuracy_score(y_test, y_pred), 3),\n",
        "            'recall': round(recall_score(y_test, y_pred), 3),\n",
        "            'prec': round(precision_score(y_test, y_pred), 3),\n",
        "            'roc_auc': round(roc_auc_score(y_test, y_pred_proba), 3),\n",
        "            'cf_matrix': confusion_matrix(y_test, y_pred),\n",
        "            'train_time': train_time_str,\n",
        "            'notes': notes}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_test_result(result):\n",
        "    test_results.loc[len(test_results)] = result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MP847vfIJMN"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6YNY0NIIL4d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/undersampled_data_60_40.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check for NaN's\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop 500+ rows containing NaN\n",
        "print(\"# of rows with NaN's before dropping:\", df.shape[0])\n",
        "df.dropna(inplace=True)\n",
        "print(\"# of rows after:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Create smaller sample from data to speed up experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_size = None\n",
        "\n",
        "# uncomment to create sample of desired size\n",
        "sample_size = 25_000\n",
        "\n",
        "if sample_size != None:\n",
        "    # ratio toxic/nontoxic\n",
        "    tox_perc = 0.4\n",
        "    nontox_perc = 0.6\n",
        "\n",
        "    # number of toxic/nontoxic rows\n",
        "    sample_size_tox = int(sample_size * tox_perc)\n",
        "    sample_size_nontox = int(sample_size * nontox_perc)\n",
        "\n",
        "    sample_tox = df[df['toxic'] == 1].sample(sample_size_tox,\n",
        "                                             random_state=42)\n",
        "    sample_nontox = df[df['toxic'] == 0].sample(sample_size_nontox,\n",
        "                                                random_state=42)\n",
        "\n",
        "    df = pd.concat([sample_tox, sample_nontox])\n",
        "    print(f'Using sample ({df.shape[0]} rows).')\n",
        "\n",
        "else:\n",
        "    print(f'Using full data ({df.shape[0]} rows).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbUc5bP0IUIS"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZffM2npRCPf"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create label/target variable and check for imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29jE5PSrPFE2"
      },
      "outputs": [],
      "source": [
        "target = df['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "value_counts = target.value_counts()\n",
        "nontoxic_count = value_counts[0]\n",
        "toxic_count = value_counts[1]\n",
        "nontoxic_perc =\\\n",
        "    round((nontoxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
        "toxic_perc =\\\n",
        "    round((toxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
        "\n",
        "print(f'Nontoxic (0): {nontoxic_count} ({nontoxic_perc} %)')\n",
        "print(f'Toxic (1): {toxic_count} ({toxic_perc} %)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create various corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spacy vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If smaller sample: Convert vector string in csv file to df\n",
        "# and cast all cols as float. This takes ~50 min for the full 360,000 rows.\n",
        "# --> If full data: Load pickle file to save time.\n",
        "\n",
        "if sample_size != None:\n",
        "    corpus_spacy = df['vector_spacy'].str.strip('[]').str.split(expand=True)\n",
        "    corpus_spacy = corpus_spacy.astype('float')\n",
        "    display(corpus_spacy)\n",
        "    # with open('pickle/spacy_vectors.pkl', mode='wb') as f:\n",
        "    #     pickle.dump(corpus_spacy, f)\n",
        "\n",
        "else:\n",
        "    with open('pickle/spacy_vectors.pkl', mode='rb') as f:\n",
        "        corpus_spacy = pickle.load(f)\n",
        "    display(corpus_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow = CountVectorizer()\n",
        "corpus_bow = vect_bow.fit_transform(df['comment_text'])\n",
        "corpus_bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# output just a small number of features, else kernel crashes while converting\n",
        "# sparse matrix to array\n",
        "n_words = 100\n",
        "pd.DataFrame(data=corpus_bow[:, 10000:10000+n_words].toarray(),\n",
        "             columns=vect_bow.get_feature_names_out()[10000:10000+n_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow_bin = CountVectorizer(binary=True)\n",
        "corpus_bow_bin = vect_bow_bin.fit_transform(df['comment_text'])\n",
        "corpus_bow_bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (mixed case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow_mixc = CountVectorizer(lowercase=False)\n",
        "corpus_bow_mixc = vect_bow_mixc.fit_transform(df['comment_text'])\n",
        "corpus_bow_mixc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of words (default) on preprocessed comments (lemmatization, stopword and punctuation removal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow_pp = CountVectorizer()\n",
        "corpus_bow_pp = vect_bow_pp.fit_transform(df['stopwords_punct_lemma'])\n",
        "corpus_bow_pp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of 1/2-grams (default) on preprocessed comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bo12grams = CountVectorizer(ngram_range=(1,2))\n",
        "corpus_bo12grams = vect_bo12grams.fit_transform(df['stopwords_punct_lemma'])\n",
        "corpus_bo12grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of 1/2/3-grams (default) on preprocessed comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bo123grams = CountVectorizer(ngram_range=(1,3))\n",
        "corpus_bo123grams = vect_bo123grams.fit_transform(df['stopwords_punct_lemma'])\n",
        "corpus_bo123grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of 2-grams (default) on preprocessed comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bo2grams = CountVectorizer(ngram_range=(2,2))\n",
        "corpus_bo2grams = vect_bo2grams.fit_transform(df['stopwords_punct_lemma'])\n",
        "corpus_bo2grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_tfidf = TfidfVectorizer()\n",
        "corpus_tfidf = vect_tfidf.fit_transform(df['comment_text'])\n",
        "corpus_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# output just a small number of features, else kernel crashes\n",
        "n_words = 100\n",
        "pd.DataFrame(data=corpus_tfidf[:, 10000:10000+n_words].toarray(),\n",
        "             columns=vect_tfidf.get_feature_names_out()[10000:10000+n_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tf_idf on preprocessed comments (lemmatization, stopword and punctuation removal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_tfidf_pp = TfidfVectorizer()\n",
        "corpus_tfidf_pp = vect_tfidf_pp.fit_transform(df['stopwords_punct_lemma'])\n",
        "corpus_tfidf_pp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline model (logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'max_iter': 2_000}\n",
        "\n",
        "# load model with parameters\n",
        "lr = LogisticRegression(**params)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words', corpus_bow, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words',\n",
        "                         corpus_bow, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words (binary)',\n",
        "                         corpus_bow_bin, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words (mixed case)',\n",
        "                         corpus_bow_mixc, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'bag of words (preprocessed)',\n",
        "                         corpus_bow_pp, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params,\n",
        "                         'bag of 1/2-grams (preprocessed)',\n",
        "                         corpus_bo12grams, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params,\n",
        "                         'bag of 1/2/3-grams (preprocessed)',\n",
        "                         corpus_bo123grams, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params,\n",
        "                         'bag of 2-grams (preprocessed)',\n",
        "                         corpus_bo2grams, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'tf_idf',\n",
        "                         corpus_tfidf, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'tf_idf (preprocessed)',\n",
        "                         corpus_tfidf_pp, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1,\n",
        "          'n_estimators': 1000}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'tf_idf (preprocessed)',\n",
        "                         corpus_tfidf_pp, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2b5q-TtHlGq",
        "outputId": "2aa1220d-5ebf-47f0-888d-3ccbf7b1f1b2"
      },
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'spacy vectors (300-D)',\n",
        "                         corpus_spacy, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1,\n",
        "          'n_estimators': 1000}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "\n",
        "test_result = test_model(xgb, 'XGBoost', params, 'spacy vectors (300-D)',\n",
        "                         corpus_spacy, target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_run_time = time.time() - full_run_time_start\n",
        "print(f'Full run time: {int(full_run_time // 60)}m {round(full_run_time % 60)}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- also try LightGBM?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
