{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/merged_data.csv', nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['comment_text', 'toxic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment_text']\n",
    "y = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42, stratify=y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data lenght check is: True - 800\n",
      "The test data lenght check is: True - 200\n"
     ]
    }
   ],
   "source": [
    "#Check for the lenght of train and test data split\n",
    "print(f\"The train data lenght check is: {X_train.shape[0] == y_train.shape[0]} - {X_train.shape[0]}\")\n",
    "print(f\"The test data lenght check is: {X_test.shape[0] == y_test.shape[0]} - {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer()\n",
    "X_train_cv = v.fit_transform(X_train.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are 3 options for Naive Bayes: GaussianNB, MultinomialNB and BernoulliNB \\nTo use GaussianNB we should check if the data has a normal distribution. Because of \\nthe assumption of the normal distribution, GaussianNB is used in cases when all our features are continuos.\\nMultinomial is used when we have discrete data. For example, rating ranging from 1 to 5.\\nIn text learning we have the count of each word to predict the class or label.\\nBernoulliNB is used when you have only 1 or 0, Binary. \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "There are 3 options for Naive Bayes: GaussianNB, MultinomialNB and BernoulliNB \n",
    "To use GaussianNB we should check if the data has a normal distribution. Because of \n",
    "the assumption of the normal distribution, GaussianNB is used in cases when all our features are continuos.\n",
    "Multinomial is used when we have discrete data. For example, rating ranging from 1 to 5.\n",
    "In text learning we have the count of each word to predict the class or label.\n",
    "BernoulliNB is used when you have only 1 or 0, Binary. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_cv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = v.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.04      0.06        56\n",
      "           1       0.72      0.97      0.82       144\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.50      0.50      0.44       200\n",
      "weighted avg       0.60      0.70      0.61       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_cv)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same now, but this time using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()), (&#x27;nb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()), (&#x27;nb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# the advantage of doing this way is that we don't need to define and use X_train_cv\n",
    "\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.04      0.06        56\n",
      "           1       0.72      0.97      0.82       144\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.50      0.50      0.44       200\n",
      "weighted avg       0.60      0.70      0.61       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to record different models performance (modified to use a pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe that will include the results\n",
    "results_table = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(clf, X_train,y_train,X_test,y_test,results_df,model_name=\"\", parameters='', comments=''):\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    predict_probab = clf.predict_proba(X_test)[:,1]\n",
    "    duration = time.time() - start_time\n",
    "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
    "\n",
    "    # Calculating all metrics\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Create a dictionary including the results\n",
    "    results = {\n",
    "        'Name': model_name if model_name else model.__class__.__name__,\n",
    "        'Parameters': parameters,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Training Time': duration_format,\n",
    "        'Comments': comments\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    new_row_df = pd.DataFrame([results])\n",
    "    # append the result to the results dataframe\n",
    "    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline you want to try\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, X_train, y_train, X_test, y_test,results_table, parameters=\"\", comments=\"Multinomial_cv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process function (Stop Words, Punctuation, Lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english language model and create nlp object from it\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Function\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "           continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "\n",
    "    return \" \".join(filtered_tokens) #this convert the list into a string separated by spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split (with preprocess corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_pp = [preprocess(text) for text in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp, X_test_pp, y_train_pp, y_test_pp = train_test_split(X_pp,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Multinomial NB with Preprocess Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline you want to try\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, X_train_pp, y_train_pp, X_test_pp, y_test_pp, results_table, parameters=\"\", comments=\"multinomial_cv_pp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Multinomial NB with Preprocess Step and Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline you want to try\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, X_train_pp, y_train_pp, X_test_pp, y_test_pp,results_table, parameters=\"bi-grams\", comments=\"multinomial_cv_pp\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline with TfidfVectorizer and MultinomialNB\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, X_train, y_train, X_test, y_test,results_table, parameters=\"\", comments=\"TfidfVectorizer\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF - IDF with Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline with TfidfVectorizer and MultinomialNB\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(min_df=5)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, X_train_pp, y_train_pp, X_test_pp, y_test_pp,results_table, parameters=\"min_df=5\", comments=\"TfidfVectorizer_pp\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data using Word Vectors Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>vector_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH yes - Were those evil Christian Missionarie...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-3.1565988, -0.40741652, -0.95940566, 0.25587...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.0249443, 1.5786849, -3.134766, -0.5616368,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.3330283, 0.32543826, -3.1496303, 0.98925835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blame men.  There's always an excuse to blame ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.3248723, 2.0334837, -1.770301, 0.16092223,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And the woman exposing herself saying grab thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2207822, 1.8134906, -1.8909769, -0.9731891,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Parliament is full of deadbeat parasites so ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.2902673, 2.660552, -3.99906, -2.1853173, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>And Billy boy did the most despicable thing to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.1255469, 2.2887607, -3.2337604, -1.9236423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I will take it! Its disgusting what you pay to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.11903482, 1.2084537, -4.553807, -2.252022, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Funny, you don't seem like a rogue Catholic to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.6919639, 1.2666397, -3.3249094, -0.6504828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>\"Obama! Benghazi! (scream it)! Clinton!\"...occ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6134049, -0.027503135, -0.24452433, -0.833...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text  toxic  \\\n",
       "0    OH yes - Were those evil Christian Missionarie...      1   \n",
       "1    Why is this black racist crap still on the G&M...      1   \n",
       "2                           even up here.......BLACKS!      1   \n",
       "3    Blame men.  There's always an excuse to blame ...      1   \n",
       "4    And the woman exposing herself saying grab thi...      1   \n",
       "..                                                 ...    ...   \n",
       "995  Parliament is full of deadbeat parasites so ho...      1   \n",
       "996  And Billy boy did the most despicable thing to...      1   \n",
       "997  I will take it! Its disgusting what you pay to...      0   \n",
       "998  Funny, you don't seem like a rogue Catholic to...      1   \n",
       "999  \"Obama! Benghazi! (scream it)! Clinton!\"...occ...      0   \n",
       "\n",
       "                                          vector_spacy  \n",
       "0    [-3.1565988, -0.40741652, -0.95940566, 0.25587...  \n",
       "1    [-1.0249443, 1.5786849, -3.134766, -0.5616368,...  \n",
       "2    [1.3330283, 0.32543826, -3.1496303, 0.98925835...  \n",
       "3    [-1.3248723, 2.0334837, -1.770301, 0.16092223,...  \n",
       "4    [0.2207822, 1.8134906, -1.8909769, -0.9731891,...  \n",
       "..                                                 ...  \n",
       "995  [-1.2902673, 2.660552, -3.99906, -2.1853173, 2...  \n",
       "996  [-1.1255469, 2.2887607, -3.2337604, -1.9236423...  \n",
       "997  [0.11903482, 1.2084537, -4.553807, -2.252022, ...  \n",
       "998  [-1.6919639, 1.2666397, -3.3249094, -0.6504828...  \n",
       "999  [-0.6134049, -0.027503135, -0.24452433, -0.833...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.copy()\n",
    "df['vector_spacy'] = df['comment_text'].apply(lambda text: nlp(text).vector)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['vector_spacy']\n",
    "y = df['toxic']\n",
    "X_train_vec, X_test_vec, y_train_vec, y_test_vec = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "X_train_2Dvec = np.stack(X_train_vec)\n",
    "X_test_2Dvec = np.stack(X_test_vec)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_embed = scaler.fit_transform(X_train_2Dvec)\n",
    "scaled_test_embed = scaler.transform(X_test_2Dvec)\n",
    "\n",
    "# Initialize the pipeline \n",
    "clf = Pipeline([\n",
    "        ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, scaled_train_embed, y_train_vec, scaled_test_embed, y_test_vec,results_table, parameters=\"\", comments=\"vectors_spacy\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and vectorization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Function\n",
    "def preprocess_and_vectorize(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "           continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "\n",
    "    return wv.get_mean_vector(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>vector_spacy</th>\n",
       "      <th>vector_ginsim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH yes - Were those evil Christian Missionarie...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-3.1565988, -0.40741652, -0.95940566, 0.25587...</td>\n",
       "      <td>[0.026361885, 0.06158323, -0.08723717, 0.08079...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.0249443, 1.5786849, -3.134766, -0.5616368,...</td>\n",
       "      <td>[0.042466734, 0.03531652, -0.04086705, 0.00051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.3330283, 0.32543826, -3.1496303, 0.98925835...</td>\n",
       "      <td>[-0.0023623807, -0.17442286, 0.06280133, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blame men.  There's always an excuse to blame ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.3248723, 2.0334837, -1.770301, 0.16092223,...</td>\n",
       "      <td>[0.037002154, 0.04110098, -0.12071987, 0.04797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And the woman exposing herself saying grab thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2207822, 1.8134906, -1.8909769, -0.9731891,...</td>\n",
       "      <td>[-0.0141696725, 0.098936565, -0.0466249, -0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Parliament is full of deadbeat parasites so ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.2902673, 2.660552, -3.99906, -2.1853173, 2...</td>\n",
       "      <td>[-0.16026868, 0.15069315, -0.08415713, 0.05298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>And Billy boy did the most despicable thing to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.1255469, 2.2887607, -3.2337604, -1.9236423...</td>\n",
       "      <td>[0.00063220196, 0.04593676, 0.034617014, 0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I will take it! Its disgusting what you pay to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.11903482, 1.2084537, -4.553807, -2.252022, ...</td>\n",
       "      <td>[0.057151046, 0.1470519, -0.040028956, 0.00079...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Funny, you don't seem like a rogue Catholic to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.6919639, 1.2666397, -3.3249094, -0.6504828...</td>\n",
       "      <td>[-0.024161603, 0.035412274, -0.024118718, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>\"Obama! Benghazi! (scream it)! Clinton!\"...occ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6134049, -0.027503135, -0.24452433, -0.833...</td>\n",
       "      <td>[-0.018149685, 0.08055272, -0.03027803, -0.024...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text  toxic  \\\n",
       "0    OH yes - Were those evil Christian Missionarie...      1   \n",
       "1    Why is this black racist crap still on the G&M...      1   \n",
       "2                           even up here.......BLACKS!      1   \n",
       "3    Blame men.  There's always an excuse to blame ...      1   \n",
       "4    And the woman exposing herself saying grab thi...      1   \n",
       "..                                                 ...    ...   \n",
       "995  Parliament is full of deadbeat parasites so ho...      1   \n",
       "996  And Billy boy did the most despicable thing to...      1   \n",
       "997  I will take it! Its disgusting what you pay to...      0   \n",
       "998  Funny, you don't seem like a rogue Catholic to...      1   \n",
       "999  \"Obama! Benghazi! (scream it)! Clinton!\"...occ...      0   \n",
       "\n",
       "                                          vector_spacy  \\\n",
       "0    [-3.1565988, -0.40741652, -0.95940566, 0.25587...   \n",
       "1    [-1.0249443, 1.5786849, -3.134766, -0.5616368,...   \n",
       "2    [1.3330283, 0.32543826, -3.1496303, 0.98925835...   \n",
       "3    [-1.3248723, 2.0334837, -1.770301, 0.16092223,...   \n",
       "4    [0.2207822, 1.8134906, -1.8909769, -0.9731891,...   \n",
       "..                                                 ...   \n",
       "995  [-1.2902673, 2.660552, -3.99906, -2.1853173, 2...   \n",
       "996  [-1.1255469, 2.2887607, -3.2337604, -1.9236423...   \n",
       "997  [0.11903482, 1.2084537, -4.553807, -2.252022, ...   \n",
       "998  [-1.6919639, 1.2666397, -3.3249094, -0.6504828...   \n",
       "999  [-0.6134049, -0.027503135, -0.24452433, -0.833...   \n",
       "\n",
       "                                         vector_ginsim  \n",
       "0    [0.026361885, 0.06158323, -0.08723717, 0.08079...  \n",
       "1    [0.042466734, 0.03531652, -0.04086705, 0.00051...  \n",
       "2    [-0.0023623807, -0.17442286, 0.06280133, -0.07...  \n",
       "3    [0.037002154, 0.04110098, -0.12071987, 0.04797...  \n",
       "4    [-0.0141696725, 0.098936565, -0.0466249, -0.04...  \n",
       "..                                                 ...  \n",
       "995  [-0.16026868, 0.15069315, -0.08415713, 0.05298...  \n",
       "996  [0.00063220196, 0.04593676, 0.034617014, 0.020...  \n",
       "997  [0.057151046, 0.1470519, -0.040028956, 0.00079...  \n",
       "998  [-0.024161603, 0.035412274, -0.024118718, -0.0...  \n",
       "999  [-0.018149685, 0.08055272, -0.03027803, -0.024...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.copy()\n",
    "df['vector_ginsim'] = df['comment_text'].apply(lambda text: preprocess_and_vectorize(text))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['vector_ginsim']\n",
    "y = df['toxic']\n",
    "X_train_vec, X_test_vec, y_train_vec, y_test_vec = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "X_train_2Dvec = np.stack(X_train_vec)\n",
    "X_test_2Dvec = np.stack(X_test_vec)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_embed = scaler.fit_transform(X_train_2Dvec)\n",
    "scaled_test_embed = scaler.transform(X_test_2Dvec)\n",
    "\n",
    "# Initialize the pipeline \n",
    "clf = Pipeline([\n",
    "        ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(clf, scaled_train_embed, y_train_vec, scaled_test_embed, y_test_vec,results_table, parameters=\"\", comments=\"vectors_ginsim\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC-ROC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td></td>\n",
       "      <td>0.824926</td>\n",
       "      <td>0.500496</td>\n",
       "      <td>0.720207</td>\n",
       "      <td>0.965278</td>\n",
       "      <td>0.705</td>\n",
       "      <td>[[  2  54]\\n [  5 139]]</td>\n",
       "      <td>0 minutes and 0.05 seconds</td>\n",
       "      <td>Multinomial_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td></td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.789189</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>0.760</td>\n",
       "      <td>[[  6  39]\\n [  9 146]]</td>\n",
       "      <td>0 minutes and 0.02 seconds</td>\n",
       "      <td>multinomial_cv_pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>bi-grams</td>\n",
       "      <td>0.864553</td>\n",
       "      <td>0.517204</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.765</td>\n",
       "      <td>[[  3  42]\\n [  5 150]]</td>\n",
       "      <td>0 minutes and 0.04 seconds</td>\n",
       "      <td>multinomial_cv_pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td></td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.720</td>\n",
       "      <td>[[  0  56]\\n [  0 144]]</td>\n",
       "      <td>0 minutes and 0.03 seconds</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>min_df=5</td>\n",
       "      <td>0.871060</td>\n",
       "      <td>0.523656</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.980645</td>\n",
       "      <td>0.775</td>\n",
       "      <td>[[  3  42]\\n [  3 152]]</td>\n",
       "      <td>0 minutes and 0.02 seconds</td>\n",
       "      <td>TfidfVectorizer_pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td></td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>[[  0  45]\\n [  0 155]]</td>\n",
       "      <td>0 minutes and 0.09 seconds</td>\n",
       "      <td>vectors_spacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td></td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>[[  0  45]\\n [  0 155]]</td>\n",
       "      <td>0 minutes and 0.0 seconds</td>\n",
       "      <td>vectors_ginsim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name Parameters  F1-Score   AUC-ROC  Precision    Recall  \\\n",
       "0  MultinomialNB             0.824926  0.500496   0.720207  0.965278   \n",
       "1  MultinomialNB             0.858824  0.537634   0.789189  0.941935   \n",
       "2  MultinomialNB   bi-grams  0.864553  0.517204   0.781250  0.967742   \n",
       "3  MultinomialNB             0.837209  0.500000   0.720000  1.000000   \n",
       "4  MultinomialNB   min_df=5  0.871060  0.523656   0.783505  0.980645   \n",
       "5  MultinomialNB             0.873239  0.500000   0.775000  1.000000   \n",
       "6  MultinomialNB             0.873239  0.500000   0.775000  1.000000   \n",
       "\n",
       "   Accuracy         Confusion Matrix               Training Time  \\\n",
       "0     0.705  [[  2  54]\\n [  5 139]]  0 minutes and 0.05 seconds   \n",
       "1     0.760  [[  6  39]\\n [  9 146]]  0 minutes and 0.02 seconds   \n",
       "2     0.765  [[  3  42]\\n [  5 150]]  0 minutes and 0.04 seconds   \n",
       "3     0.720  [[  0  56]\\n [  0 144]]  0 minutes and 0.03 seconds   \n",
       "4     0.775  [[  3  42]\\n [  3 152]]  0 minutes and 0.02 seconds   \n",
       "5     0.775  [[  0  45]\\n [  0 155]]  0 minutes and 0.09 seconds   \n",
       "6     0.775  [[  0  45]\\n [  0 155]]   0 minutes and 0.0 seconds   \n",
       "\n",
       "             Comments  \n",
       "0      Multinomial_cv  \n",
       "1   multinomial_cv_pp  \n",
       "2   multinomial_cv_pp  \n",
       "3     TfidfVectorizer  \n",
       "4  TfidfVectorizer_pp  \n",
       "5       vectors_spacy  \n",
       "6      vectors_ginsim  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
