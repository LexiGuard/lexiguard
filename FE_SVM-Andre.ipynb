{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABw20b-PQ85_"
      },
      "source": [
        "# Get data Ready!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N_rows and percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tUfyPPxGR3Zz"
      },
      "outputs": [],
      "source": [
        "# Decide how many lines you want to run and the % of it you want to use\n",
        "# Total lines in the file: 360835\n",
        "n_rows = 360000\n",
        "percentage_rows = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKtKyxK8gnhP"
      },
      "source": [
        "## Importing and setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q54yPL0WgUle",
        "outputId": "f7bf0ac6-d1cc-4a90-d331-b4c63eb078ff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm # progress bar\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import spacy # (object oriented)\n",
        "import nltk # natural language tool kit (string oriented)\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "#Run this lines only once\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy download en_core_web_md\n",
        "#!python -m spacy download en_core_web_lg #587.7 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('data/undersampled_data_60_40.csv', nrows=n_rows)\n",
        "# copy data\n",
        "df = data.copy()\n",
        "# Using only # % of datset\n",
        "df = df.sample(frac=percentage_rows / 100, random_state=42)\n",
        "# before train_split:\n",
        "df = df.dropna(subset=['stopwords_punct_lemma'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Count Percentage\n",
            "toxic                  \n",
            "0      21523     59.87%\n",
            "1      14426     40.13%\n"
          ]
        }
      ],
      "source": [
        "# Get counts and percentages\n",
        "counts, percentages = df['toxic'].value_counts(), df['toxic'].value_counts(normalize=True) * 100\n",
        "# Display counts and percentages\n",
        "result_df = pd.DataFrame({'Count': counts, 'Percentage': percentages})\n",
        "result_df['Percentage'] = result_df['Percentage'].map('{:.2f}%'.format)\n",
        "print(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>stopwords_punct_lemma</th>\n",
              "      <th>vector_spacy</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>pos_tags_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76637</th>\n",
              "      <td>If only the clintons were held to the same sta...</td>\n",
              "      <td>0</td>\n",
              "      <td>clinton hold standard press hold trump agree</td>\n",
              "      <td>[-1.2594286   1.6080885  -2.1067002   2.376314...</td>\n",
              "      <td>[('If', 'IN'), ('only', 'RB'), ('the', 'DT'), ...</td>\n",
              "      <td>IN RB DT NNS VBD VBN TO DT JJ NN IN DT NN VBZ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64372</th>\n",
              "      <td>Time to bake the doughboy.</td>\n",
              "      <td>0</td>\n",
              "      <td>time bake doughboy</td>\n",
              "      <td>[ 2.0744269e-01  2.2579332e-01 -4.8536677e-02 ...</td>\n",
              "      <td>[('Time', 'NN'), ('to', 'TO'), ('bake', 'VB'),...</td>\n",
              "      <td>NN TO VB DT NN .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252387</th>\n",
              "      <td>Yeah, but they're white. It's totally cool and...</td>\n",
              "      <td>1</td>\n",
              "      <td>yeah white totally cool okay discriminate whit...</td>\n",
              "      <td>[-2.457021    0.44777998 -3.158751    0.608898...</td>\n",
              "      <td>[('Yeah', 'UH'), (',', ','), ('but', 'CC'), ('...</td>\n",
              "      <td>UH , CC PRP VBP JJ . PRP VBZ RB JJ CC JJ TO VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251573</th>\n",
              "      <td>I agree with you, but his choice of language i...</td>\n",
              "      <td>1</td>\n",
              "      <td>agree choice language damnable find way expres...</td>\n",
              "      <td>[-0.39756107  1.039007   -0.66458005 -1.212819...</td>\n",
              "      <td>[('I', 'PRP'), ('agree', 'VBP'), ('with', 'IN'...</td>\n",
              "      <td>PRP VBP IN PRP , CC PRP$ NN IN NN VBZ JJ . PRP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318742</th>\n",
              "      <td>The false theory of evolution is facts?   I am...</td>\n",
              "      <td>0</td>\n",
              "      <td>false theory evolution fact    advocate school...</td>\n",
              "      <td>[-0.7768647   0.87667793 -1.2914572  -0.622957...</td>\n",
              "      <td>[('The', 'DT'), ('false', 'JJ'), ('theory', 'N...</td>\n",
              "      <td>DT JJ NN IN NN VBZ NNS . PRP VBP VBG IN NN NN ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment_text  toxic  \\\n",
              "76637   If only the clintons were held to the same sta...      0   \n",
              "64372                          Time to bake the doughboy.      0   \n",
              "252387  Yeah, but they're white. It's totally cool and...      1   \n",
              "251573  I agree with you, but his choice of language i...      1   \n",
              "318742  The false theory of evolution is facts?   I am...      0   \n",
              "\n",
              "                                    stopwords_punct_lemma  \\\n",
              "76637        clinton hold standard press hold trump agree   \n",
              "64372                                  time bake doughboy   \n",
              "252387  yeah white totally cool okay discriminate whit...   \n",
              "251573  agree choice language damnable find way expres...   \n",
              "318742  false theory evolution fact    advocate school...   \n",
              "\n",
              "                                             vector_spacy  \\\n",
              "76637   [-1.2594286   1.6080885  -2.1067002   2.376314...   \n",
              "64372   [ 2.0744269e-01  2.2579332e-01 -4.8536677e-02 ...   \n",
              "252387  [-2.457021    0.44777998 -3.158751    0.608898...   \n",
              "251573  [-0.39756107  1.039007   -0.66458005 -1.212819...   \n",
              "318742  [-0.7768647   0.87667793 -1.2914572  -0.622957...   \n",
              "\n",
              "                                                 pos_tags  \\\n",
              "76637   [('If', 'IN'), ('only', 'RB'), ('the', 'DT'), ...   \n",
              "64372   [('Time', 'NN'), ('to', 'TO'), ('bake', 'VB'),...   \n",
              "252387  [('Yeah', 'UH'), (',', ','), ('but', 'CC'), ('...   \n",
              "251573  [('I', 'PRP'), ('agree', 'VBP'), ('with', 'IN'...   \n",
              "318742  [('The', 'DT'), ('false', 'JJ'), ('theory', 'N...   \n",
              "\n",
              "                                             pos_tags_str  \n",
              "76637   IN RB DT NNS VBD VBN TO DT JJ NN IN DT NN VBZ ...  \n",
              "64372                                    NN TO VB DT NN .  \n",
              "252387  UH , CC PRP VBP JJ . PRP VBZ RB JJ CC JJ TO VB...  \n",
              "251573  PRP VBP IN PRP , CC PRP$ NN IN NN VBZ JJ . PRP...  \n",
              "318742  DT JJ NN IN NN VBZ NNS . PRP VBP VBG IN NN NN ...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPCIWO4TNYLp"
      },
      "source": [
        "## Evaluation Func that fits, predict and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize dataframe that will include the results\n",
        "results_table = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xTJSGXBYNdMb"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_train,y_train,X_test,y_test,results_df,model_name=\"\", parameters='', comments=''):\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    predict_probab = model.predict_proba(X_test)[:,1]\n",
        "    duration = time.time() - start_time\n",
        "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
        "\n",
        "    # Calculating all metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "    roc_auc = roc_auc_score(y_test, predict_probab)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
        "\n",
        "    # Create a dictionary including the results\n",
        "    results = {\n",
        "        'Name': model_name if model_name else model.__class__.__name__,\n",
        "        'Parameters': parameters,\n",
        "        'F1-Score': f1,\n",
        "        'AUC-ROC': roc_auc,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'Accuracy': accuracy,\n",
        "        'Confusion Matrix': conf_matrix,\n",
        "        'Training Time': duration_format,\n",
        "        'Comments': comments\n",
        "    }\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    new_row_df = pd.DataFrame([results])\n",
        "    # don't forget to append the result to the results dataframe\n",
        "    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_clean(model, X_train,y_train,X_test,y_test,results_df,model_name=\"\", parameters='', comments=''):\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    #predict_probab = model.predict_proba(X_test)[:,1]\n",
        "    duration = time.time() - start_time\n",
        "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
        "\n",
        "    # Calculating all metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "    #roc_auc = roc_auc_score(y_test, predict_probab)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
        "\n",
        "    # Create a dictionary including the results\n",
        "    results = {\n",
        "        'Name': model_name if model_name else model.__class__.__name__,\n",
        "        'Parameters': parameters,\n",
        "        'F1-Score': f1,\n",
        "        #'AUC-ROC': roc_auc,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'Accuracy': accuracy,\n",
        "        'Confusion Matrix': conf_matrix,\n",
        "        'Training Time': duration_format,\n",
        "        'Comments': comments\n",
        "    }\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    new_row_df = pd.DataFrame([results])\n",
        "    # don't forget to append the result to the results dataframe\n",
        "    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion of spacy vector from string to numerical value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_string_to_array(string):\n",
        "    try:\n",
        "        # Trying to convert using ast.literal_eval\n",
        "        return np.array(ast.literal_eval(string))\n",
        "    except:\n",
        "        # If ast.literal_eval fails, use an alternative method\n",
        "        # Remove brackets and split the string by space\n",
        "        cleaned_string = string.strip('[]')\n",
        "        split_strings = cleaned_string.split()\n",
        "        # Convert each split string to float and then to a numpy array\n",
        "        return np.array([float(i) for i in split_strings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"# Apply the function\\ndf['vector_spacy'] = df['vector_spacy'].apply(convert_string_to_array)\\n# Check the result\\nprint(df['vector_spacy'][0][0], type(df['vector_spacy'][0][0]))\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Apply the function\n",
        "df['vector_spacy'] = df['vector_spacy'].apply(convert_string_to_array)\n",
        "# Check the result\n",
        "print(df['vector_spacy'][0][0], type(df['vector_spacy'][0][0]))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK - SVM - stopwords_punct_lemma - Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_stop = df['stopwords_punct_lemma']\n",
        "y_stop = df['toxic']\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform text data into TF-IDF features\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X_stop)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_tfidf, y_stop, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "model = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Call the evaluate_model function\n",
        "results_table = evaluate_model_clean(model, X_train_s, y_train_s, X_test_s, y_test_s,\n",
        "                                     results_table, model_name=\"SVC\",parameters=\"\", \n",
        "                                     comments=\"SVM - stopwords_punct_lemma - TFIDf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Parameters</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Confusion Matrix</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVC</td>\n",
              "      <td></td>\n",
              "      <td>0.805718</td>\n",
              "      <td>0.858594</td>\n",
              "      <td>0.758978</td>\n",
              "      <td>0.852573</td>\n",
              "      <td>[[3932  362]\\n [ 698 2198]]</td>\n",
              "      <td>6 minutes and 19.54 seconds</td>\n",
              "      <td>SVM - stopwords_punct_lemma - TFIDf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Name Parameters  F1-Score  Precision    Recall  Accuracy  \\\n",
              "0  SVC             0.805718   0.858594  0.758978  0.852573   \n",
              "\n",
              "              Confusion Matrix                Training Time  \\\n",
              "0  [[3932  362]\\n [ 698 2198]]  6 minutes and 19.54 seconds   \n",
              "\n",
              "                              Comments  \n",
              "0  SVM - stopwords_punct_lemma - TFIDf  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK -  SVM -  stopwords_punct_lemma - word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = [text.split() for text in df['stopwords_punct_lemma']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Transform each text into an average Word2Vec vector\n",
        "word2vec_vectors = []\n",
        "for text in sentences:\n",
        "    vectors = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv]\n",
        "    if vectors:\n",
        "        text_vector = np.mean(vectors, axis=0)\n",
        "        word2vec_vectors.append(text_vector)\n",
        "    else:\n",
        "        # Handle cases where there are no words found in the Word2Vec model\n",
        "        word2vec_vectors.append(np.zeros(word2vec_model.vector_size))\n",
        "\n",
        "# Convert the list of Word2Vec vectors into a matrix\n",
        "X_word2vec = np.vstack(word2vec_vectors)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_word2vec, y_stop, \n",
        "                                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "svm_model_w2v = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Call the evaluate_model function for Word2Vec\n",
        "results_table = evaluate_model_clean(svm_model_w2v, X_train_w2v, y_train_w2v, X_test_w2v, y_test_w2v,\n",
        "                                     results_table, model_name=\"SVC\",parameters=\"\",\n",
        "                                     comments=\"SVM - stopwords_punct_lemma - Word2Vec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK - SVM - stopwords_punct_lemma - count vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_c = df['stopwords_punct_lemma']\n",
        "y_c = df['toxic']\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Transform text data into CountVectorized features\n",
        "X_count = count_vectorizer.fit_transform(X_c)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_count, y_c, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "svm_model_c = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Call the evaluate_model function for CountVectorizer\n",
        "results_table = evaluate_model_clean(svm_model_c, X_train_c, y_train_c, X_test_c, y_test_c,\n",
        "                                     results_table, model_name=\"SVM\", parameters=\"\", \n",
        "                                     comments=\"SVM - stopwords_punct_lemma - CountVec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK - SVM + POS + count vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['pos_tags_str'])\n",
        "y = df['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Use the evaluate_model function \n",
        "results_table = evaluate_model_clean(svm_model, X_train, y_train, X_test, y_test,\n",
        "                          results_table,model_name=\"SVM\",parameters=\"\",\n",
        "                          comments=\"POS + RFC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK - SVM + POS + TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_pos_tf = vectorizer.fit_transform(df['pos_tags_str'])\n",
        "y_pos_tf = df['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train_pos_tf, X_test_pos_tf, y_train_pos_tf, y_test_pos_tf = train_test_split(X_pos_tf, y_pos_tf, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Use the evaluate_model function to assess the model\n",
        "results_table = evaluate_model_clean(svm_model, X_train_pos_tf, y_train_pos_tf, X_test_pos_tf, y_test_pos_tf,\n",
        "                         results_table, model_name=\"SVM\", parameters=\"\", \n",
        "                         comments=\"POS + using TF-IDF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK - TF-IDF - SVM ( standard + best param 1 and 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate data into features (X) and labels (y)\n",
        "X = df['comment_text']\n",
        "y = df['toxic']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM standart + TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply tfidf vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=8000, lowercase=True, stop_words='english')\n",
        "\n",
        "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='linear')#, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model_clean(svm_model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"SVM_tfidf\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM best param 1 + TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Best Parameters 1: {'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf', 'tfidf__max_df': 0.85, 'tfidf__max_features': 3000, 'tfidf__min_df': 5}\n",
        "\n",
        "# apply tfidf vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000, min_df=5, max_df=0.85, lowercase=True, stop_words='english')\n",
        "\n",
        "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.1)#, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model_clean(svm_model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Best 1 SVM_tfidf\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM best param 2 + TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Best Parameters 2: {'svm__C': 10, 'svm__gamma': 0.7, 'svm__kernel': 'rbf', 'tfidf__max_df': 0.85, 'tfidf__max_features': 3000, 'tfidf__min_df': 10}\n",
        "\n",
        "# apply tfidf vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000, min_df=10, max_df=0.85, lowercase=True, stop_words='english')\n",
        "\n",
        "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model_clean(svm_model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Best 2 SVM_tfidf\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OK - SVM + spacy vectorizer (takes too long to run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_vect = df['vector_spacy'].str.strip('[]').str.split(expand=True)\n",
        "corpus_vect = corpus_vect.astype('float')\n",
        "\n",
        "df['new_vector_spacy'] = corpus_vect.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_sp = np.array(df['new_vector_spacy'].tolist())\n",
        "y_sp = df['toxic']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_sp, X_test_sp, y_train_sp, y_test_sp = train_test_split(X_sp, y_sp, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7)#, probability=True)\n",
        "\n",
        "# Call the evaluate_model\n",
        "results_table = evaluate_model_clean(svm_model, X_train_sp, y_train_sp, X_test_sp, y_test_sp, \n",
        "                                     results_table, model_name=\"SVM\", parameters=\"\", \n",
        "                                     comments=\"SVM - SpaCy vec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Working but not being used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUrGFfCGUW-Z"
      },
      "source": [
        "## Bag of Words (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfZDVQ0EULrl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Fit the CountVectorizer to the training data\n",
        "vect = CountVectorizer().fit(X_train)\n",
        "\n",
        "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "# Prepare X_test for the function\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', probability=True, C=10, gamma=0.7,)\n",
        "\n",
        "# Call the function and store the row in the variable result\n",
        "results_table = evaluate_model(svm_model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Bag - Baseline\" )\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50BbC72pUqXa"
      },
      "source": [
        "## Bag of words Binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BthUDYVXUUVG"
      },
      "outputs": [],
      "source": [
        "'''#Fit the CountVectorizer to the training data\n",
        "vect = CountVectorizer(binary=True).fit(X_train)\n",
        "\n",
        "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "# Prepare X_test for the function\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', probability=True, C=10, gamma=0.7,)\n",
        "\n",
        "# Call the function and store the row in the variable result\n",
        "results_table = evaluate_model(svm_model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Bag - Binary\" )'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8c7OAVYU1KN"
      },
      "source": [
        "## Bag of Words (Binary + Stop Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sDFb8ZkU1r_"
      },
      "outputs": [],
      "source": [
        "'''stop_words = set(stopwords.words('english'))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xabl9lnU76L"
      },
      "outputs": [],
      "source": [
        "'''#Fit the CountVectorizer to the training data\n",
        "vect = CountVectorizer(binary=True, stop_words=list(stop_words)).fit(X_train)\n",
        "\n",
        "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "# Prepare X_test for the function\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', probability=True, C=10, gamma=0.7,)\n",
        "\n",
        "# Call the function and store the row in the variable result\n",
        "results_table = evaluate_model(svm_model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Bag - Binary/StopWords\" )'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc1jRLkWVIeK"
      },
      "source": [
        "## Stemming(Bag of words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7nOfV2UVGJT"
      },
      "outputs": [],
      "source": [
        "'''# Initializing stemmer and countvectorizer\n",
        "stemmer = nltk.PorterStemmer()\n",
        "cv_analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    \n",
        "    #In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
        "    \n",
        "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
        "\n",
        "# define CountVectorizer with stemming function\n",
        "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
        "\n",
        "# Prepare X_train for the function\n",
        "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Prepare X_test for the function\n",
        "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"\", comments=\"Stem + bag + svm\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVWXB9JVVg7S"
      },
      "source": [
        "## Stemming(Bag of words(stopwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtQEE3O2WZQr"
      },
      "outputs": [],
      "source": [
        "'''# Initializing stemmer and countvectorizer with Stop Words\n",
        "stemmer = nltk.PorterStemmer()\n",
        "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    \n",
        "    #In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
        "    \n",
        "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
        "\n",
        "# define CountVectorizer with stemming function\n",
        "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
        "\n",
        "# Prepare X_train for the function\n",
        "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Prepare X_test for the function\n",
        "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"\", comments=\"Stem + bag + stop\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc0Gk2eEVlBj"
      },
      "source": [
        "## Stemming with TF - IDF and stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew7LDurKWZxD"
      },
      "outputs": [],
      "source": [
        "'''stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# stop_words contains a list of 179 words that we want to remove from our comments\n",
        "\n",
        "# Initializing stemmer and countvectorizer with Stop Words\n",
        "stemmer = nltk.PorterStemmer()\n",
        "tfidf_analyzer = TfidfVectorizer(min_df=30, stop_words=list(stop_words)).build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    \n",
        "    #In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
        "    \n",
        "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
        "\n",
        "# define CountVectorizer with stemming function\n",
        "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
        "\n",
        "# Prepare X_train for the function\n",
        "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Prepare X_test for the function\n",
        "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"\", comments=\"Stem - tfidf - stop\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHJB8n_CVp8s"
      },
      "source": [
        "## Lemmatization with Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmhxMFekWanZ"
      },
      "outputs": [],
      "source": [
        "'''# Initialization\n",
        "WNlemma = nltk.WordNetLemmatizer()\n",
        "cv_analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "def lemmatize_word(doc):\n",
        "    \n",
        "    #In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
        "    \n",
        "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
        "\n",
        "# define CountVectorizer with Lemmatization function\n",
        "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
        "\n",
        "# Prepare X_train for the function\n",
        "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
        "# Prepare X_test for the function\n",
        "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"\", comments=\"lem + bag\")\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdYtRs_5V9P9"
      },
      "source": [
        "## Lemmatization with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92RlzKhCWbKU"
      },
      "outputs": [],
      "source": [
        "'''# Initialization\n",
        "WNlemma = nltk.WordNetLemmatizer()\n",
        "cv_analyzer = TfidfVectorizer(min_df=30).build_analyzer()\n",
        "\n",
        "def lemmatize_word(doc):\n",
        "    \n",
        "    \n",
        "    #In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
        "    \n",
        "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
        "\n",
        "# define CountVectorizer with Lemmatization function\n",
        "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
        "\n",
        "# Prepare X_train for the function\n",
        "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
        "# Prepare X_test for the function\n",
        "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"\", comments=\"lem - tfidf\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2E-Gf00TBUz"
      },
      "source": [
        "## Lemm with StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG9XEY-DTDEp"
      },
      "outputs": [],
      "source": [
        "'''# Initialization\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "WNlemma = nltk.WordNetLemmatizer()\n",
        "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
        "\n",
        "def lemmatize_word(doc):\n",
        "    \n",
        "    #In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
        "    \n",
        "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
        "\n",
        "# define CountVectorizer with Lemmatization function\n",
        "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
        "\n",
        "# Prepare X_train for the function\n",
        "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
        "# Prepare X_test for the function\n",
        "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"\", comments=\"lem - stop\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmksXbnjW37r"
      },
      "source": [
        "## Word Vectors - Spacy library - Small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW0uTskzW7KI"
      },
      "outputs": [],
      "source": [
        "'''# This initialize a pre-trained model (the small version) that uses Neural Networks to build word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# convert words into vectors and Prepare X_train for the function\n",
        "docs = [nlp(text) for text in X_train]\n",
        "X_train_word_vectors = [x.vector for x in docs]\n",
        "\n",
        "# Prepare X_test for the function\n",
        "docs_test = [nlp(text) for text in X_test]\n",
        "X_test_word_vectors = [x.vector for x in docs_test]\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_sm\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ71Rd6zW7sk"
      },
      "source": [
        "## Word Vectors - Spacy library - Medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# convert vector string to df and cast all cols as float\n",
        "corpus_vect = df['vector_spacy'].str.strip('[]').str.split(expand=True)\n",
        "corpus_vect = corpus_vect.astype('float')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CEkeGpAW9pK"
      },
      "outputs": [],
      "source": [
        "'''# This initialize a pre-trained model (the medium version) that uses Neural Networks to build word vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# convert words into vectors and Prepare X_train for the function\n",
        "docs = [nlp(text) for text in X_train]\n",
        "X_train_word_vectors = [x.vector for x in docs]\n",
        "\n",
        "# Prepare X_test for the function\n",
        "docs_test = [nlp(text) for text in X_test]\n",
        "X_test_word_vectors = [x.vector for x in docs_test]\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_md\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# Separate data into features (X) and labels (y)\n",
        "X = df['vector_spacy']\n",
        "y = df['toxic']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train_word_vectors, X_test_word_vectors, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_md\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfI-2XAzW-WA"
      },
      "source": [
        "## Word Vectors - Spacy library - Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXzOjAPyW_05"
      },
      "outputs": [],
      "source": [
        "'''# This initialize a pre-trained model (the large version) that uses Neural Networks to build word vectors\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# convert words into vectors and Prepare X_train for the function\n",
        "docs = [nlp(text) for text in X_train]\n",
        "X_train_word_vectors = [x.vector for x in docs]\n",
        "\n",
        "# Prepare X_test for the function\n",
        "docs_test = [nlp(text) for text in X_test]\n",
        "X_test_word_vectors = [x.vector for x in docs_test]\n",
        "\n",
        "# Instantiate\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.7, probability=True)\n",
        "\n",
        "# Fit, predict and evaluate\n",
        "results_table = evaluate_model(svm_model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_lg\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "A0NYgd5xXCQC",
        "outputId": "ce888217-3382-4c67-a001-0e0faa46ef21"
      },
      "outputs": [],
      "source": [
        "# results_table"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ABw20b-PQ85_",
        "K4WLd55IOVGy",
        "du7KTpc7S3J6",
        "cUrGFfCGUW-Z",
        "50BbC72pUqXa",
        "e8c7OAVYU1KN",
        "kc1jRLkWVIeK",
        "ZVWXB9JVVg7S",
        "gc0Gk2eEVlBj",
        "VHJB8n_CVp8s",
        "fdYtRs_5V9P9",
        "d2E-Gf00TBUz",
        "KmksXbnjW37r",
        "wQ71Rd6zW7sk",
        "FfI-2XAzW-WA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
