{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Includes code from Purvi, Eric, André, Michael. Merge done by Michael, reviewed by group.\n",
    "\n",
    "@all:  \n",
    "I use a max code line length of 79, as suggested by PEP-8, as this is useful when you want to compare code side by side (https://github.com/python/peps/blob/main/peps/pep-0008.rst). You can setup VS Code to show a ruler/marker (or several of them) to help keep line length in check: https://levelup.gitconnected.com/do-you-know-about-rulers-in-visual-studio-code-f754b221a135. Nice to have, not a must. I'm open for any discussions about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual suspects\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nltk (Natural Language Toolkit, https://www.nltk.org)\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# download stuff for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# other imports\n",
    "from wordcloud import WordCloud # https://pypi.org/project/wordcloud/\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import missingno as msno\n",
    "\n",
    "# @André and all:\n",
    "# I leave warnings switched on. They can be annoying, yes, but I think\n",
    "# they are usually thrown for a good reason and it's probably always\n",
    "# better to write code that doesn't produce them in order to avoid\n",
    "# problems further down the road.\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# increase number of displayed df columns, since data has quite many\n",
    "# (default is 20)\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data.csv contains only complete rows (~ 450,000 of a total 2M)\n",
    "# Target: 'toxic' column (1 if 'toxicity' >= 0.5)\n",
    "df = pd.read_csv('data/merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_cols = ['id', 'comment_text', 'split', 'toxicity']\n",
    "\n",
    "subtype_cols = ['severe_toxicity', 'obscene', 'insult', 'threat',\n",
    "                'identity_attack','sexual_explicit']\n",
    "\n",
    "metadata_cols = ['created_date', 'publication_id', 'parent_id',\n",
    "                 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes',\n",
    "                 'disagree', 'identity_annotator_count',\n",
    "                 'toxicity_annotator_count']\n",
    "\n",
    "identity_cols = ['male', 'female', 'transgender', 'other_gender',\n",
    "                 'heterosexual', 'homosexual_gay_or_lesbian',\n",
    "                 'bisexual', 'other_sexual_orientation', 'christian',\n",
    "                 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist',\n",
    "                 'other_religion', 'black', 'white', 'asian', 'latino',\n",
    "                 'other_race_or_ethnicity', 'physical_disability',\n",
    "                 'intellectual_or_learning_disability',\n",
    "                 'psychiatric_or_mental_illness', 'other_disability']\n",
    "\n",
    "# further split up identity columns\n",
    "gender_cols = ['male', 'female', 'bisexual', 'transgender', 'heterosexual',\n",
    "               'other_gender', 'homosexual_gay_or_lesbian',\n",
    "               'other_sexual_orientation']\n",
    "\n",
    "race_cols = ['asian', 'black', 'white', 'latino', 'other_race_or_ethnicity']\n",
    "\n",
    "religion_cols = ['hindu', 'buddhist', 'christian', 'muslim', 'jewish',\n",
    "                 'atheist', 'other_religion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "print('Total # of NaNs in dataset:', df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of NaNs using Missingno\n",
    "msno.bar(df)\n",
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check string columns other then comment_text. What's in there?\n",
    "print('Column \"rating\":', df['rating'].unique())\n",
    "print('Column \"split\":', df['split'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations (Michael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic vs non-toxic comments (pie plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_count = df['toxic'].value_counts()\n",
    "toxic_count.plot.pie(labels=['Non-toxic', 'Toxic'], autopct='%.1f%%')\n",
    "plt.title('Comments')\n",
    "plt.ylabel(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> __Dataset is strongly imbalanced!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of toxicity (histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxicity'].hist(bins=30)\n",
    "plt.title('Distribution of toxicity')\n",
    "plt.xlabel('Toxicity')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms for all numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=30, figsize=(30, 30));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate percentage for each identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new binary identity features/columns (1 if value >= 0.5)\n",
    "bin_identity_cols = []\n",
    "for identity in identity_cols:\n",
    "    new_col = 'is_' + identity\n",
    "    df[new_col] = (df[identity] >= 0.5).astype(int)\n",
    "    bin_identity_cols.append(new_col)\n",
    "\n",
    "print('# of identity columns:', len(identity_cols))\n",
    "print('# of new binary identity columns:', len(bin_identity_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage for each identity\n",
    "id_percentages = {}\n",
    "for identity in bin_identity_cols:\n",
    "    cnt = df[['id', identity]].groupby(identity).count()['id']\n",
    "    perc = (cnt[1] / len(df) * 100).round(3)\n",
    "    id_percentages[identity] = perc\n",
    "\n",
    "print(id_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dict into Pandas Series\n",
    "id_perc_ser = pd.Series(id_percentages).sort_values()\n",
    "\n",
    "# drop 'is_' from index labels\n",
    "id_perc_ser.rename(lambda x: x.replace('is_', ''), inplace=True)\n",
    "\n",
    "# plot\n",
    "id_perc_ser.plot.barh()\n",
    "plt.title('Identity percentages')\n",
    "plt.xlabel('Share of total observations (percent)')\n",
    "plt.ylabel('Identity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations (Eric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud (toxic comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with only text and target\n",
    "df_only_text = df[['comment_text','toxic']]\n",
    "\n",
    "toxic_comments = df_only_text[df_only_text['toxic'] == 1]['comment_text']\n",
    "nontoxic_comments = df_only_text[df_only_text['toxic'] == 0]['comment_text']\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\\\n",
    "    .generate(\"\".join(toxic_comments))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off'); # semicolon suppresses ugly line of code over plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud (non-toxic comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_non_toxic = WordCloud(width=800, height=400,\n",
    "    background_color='white')\\\n",
    "    .generate(\"\".join(nontoxic_comments))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_non_toxic, interpolation='bilinear')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_tokens = toxic_comments.apply(word_tokenize)\n",
    "nontoxic_tokens = nontoxic_comments.apply(word_tokenize)\n",
    "\n",
    "toxic_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation\n",
    "\n",
    "Filter out non-alphabetical tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_tokens = toxic_tokens.apply(\n",
    "    lambda x: [word for word in x if word.isalpha()])\n",
    "nontoxic_tokens = nontoxic_tokens.apply(\n",
    "    lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "toxic_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to lower case\n",
    "To ensure uniformity we have to convert all words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_tokens = toxic_tokens.apply(lambda x: [word.lower() for word in x])\n",
    "nontoxic_tokens = nontoxic_tokens.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "toxic_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords\n",
    "This will remove common words like: this, is, and, the, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "                         \n",
    "# stop_words contains a list of 179 words that we want to remove from our\n",
    "# comments\n",
    "\n",
    "toxic_tokens = toxic_tokens.apply(\n",
    "    lambda x: [word for word in x if word not in stop_words])\n",
    "nontoxic_tokens = nontoxic_tokens.apply(\n",
    "    lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "toxic_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemmatization reduces words to their base or dictionary form. It's usually more sophisticated than stemming.\n",
    "\n",
    "@Eric:  \n",
    "Does is make sense to do both lemmatization and stemming as you did below? Isn't this mutually exclusive? Just a thought, not sure ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized = []\n",
    "    for word in tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, pos=wordnet.VERB))\n",
    "    return lemmatized\n",
    "\n",
    "toxic_tokens = toxic_tokens.apply(lemmatize_tokens)\n",
    "nontoxic_tokens = nontoxic_tokens.apply(lemmatize_tokens)\n",
    "\n",
    "toxic_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming reduces words to their word stem or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "toxic_tokens = toxic_tokens.apply(\n",
    "    lambda x: [stemmer.stem(word) for word in x])\n",
    "nontoxic_tokens = nontoxic_tokens.apply(\n",
    "    lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "toxic_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create flat token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_token_list = []\n",
    "for sublist in toxic_tokens:\n",
    "    for token in sublist:\n",
    "        flat_token_list.append(token)\n",
    "\n",
    "len(flat_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token frequency (line plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(flat_token_list)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "fdist.plot(50, cumulative=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token frequency top 20 (bar plot with highlighted bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame(flat_token_list, columns=['word'])['word']\\\n",
    "    .value_counts()\\\n",
    "    .head(50)\n",
    "\n",
    "# define words to highlight\n",
    "highlight = ['black', 'women', 'man', 'trump', 'white', 'muslim',\n",
    "             'men', 'gay', 'christian']\n",
    "\n",
    "# create color array\n",
    "colors = ['red' if word in highlight else 'blue' for word in word_counts.index]\n",
    "\n",
    "# plot bar chart\n",
    "plt.figure(figsize=(15, 10))\n",
    "word_counts.plot(kind='bar', color=colors)\n",
    "plt.title('Top 20 Most Frequent Tokens with Highlights')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Tokens')\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bi-grams (2-grams)\n",
    "bi_grams = ngrams(flat_token_list, 2)\n",
    "bi_gram_counts = Counter(bi_grams)\n",
    "\n",
    "# Generate tri-grams (3-grams)\n",
    "tri_grams = ngrams(flat_token_list, 3)\n",
    "tri_gram_counts = Counter(tri_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print top 10 n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 10 most common bi-grams\n",
    "print(\"Most Common Bi-grams:\")\n",
    "for gram, count in bi_gram_counts.most_common(10):\n",
    "    print(f\"{gram}: {count}\")\n",
    "\n",
    "# Print the 10 most common tri-grams\n",
    "print(\"\\nMost Common Tri-grams:\")\n",
    "for gram, count in tri_gram_counts.most_common(10):\n",
    "    print(f\"{gram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot top 10 n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the bi-gram and tri-gram counts to DataFrames\n",
    "df_bi_grams = pd.DataFrame(bi_gram_counts.most_common(10),\n",
    "                           columns=['bi_gram', 'count'])\n",
    "df_tri_grams = pd.DataFrame(tri_gram_counts.most_common(10),\n",
    "                            columns=['tri_gram', 'count'])\n",
    "\n",
    "# Plot bi-grams\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(df_bi_grams['bi_gram'].astype(str), df_bi_grams['count'])\n",
    "plt.title('Top 10 Most Common Bi-grams')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Plot tri-grams\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(df_tri_grams['tri_gram'].astype(str), df_tri_grams['count'])\n",
    "plt.title('Top 10 Most Common Tri-grams')\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations (Purvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = df[df['toxic'] == 1] # toxic data have values where toxic = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap using toxic = 1 and columns >= 0.5\n",
    "\n",
    "t_gend = (toxic_data[gender_cols].values >= 0.5).any(axis=1)\n",
    "corr_gender = toxic_data.loc[t_gend, gender_cols + ['toxicity']]\n",
    "\n",
    "t_race = (toxic_data[race_cols].values >= 0.5).any(axis=1)\n",
    "corr_race = toxic_data.loc[t_race, race_cols + ['toxicity']]\n",
    "\n",
    "t_rel = (toxic_data[religion_cols].values >= 0.5).any(axis=1)\n",
    "corr_rel = toxic_data.loc[t_rel, religion_cols + ['toxicity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Purvi:  \n",
    "Just a suggestion: Should we hide the \"upper triangle\" above the main diagonal? I feel that the chart becomes more readable this way. Maybe you want/can look into that if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr_gender_matrix = corr_gender.corr()\n",
    "corr_race_matrix = corr_race.corr()\n",
    "corr_rel_matrix = corr_rel.corr()\n",
    "\n",
    "# Create subplot for each heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot correlation heatmaps\n",
    "sns.heatmap(corr_gender_matrix, annot=True, cmap='coolwarm',\n",
    "            fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Correlation Heatmap - Gender')\n",
    "\n",
    "sns.heatmap(corr_race_matrix, annot=True, cmap='coolwarm',\n",
    "            fmt='.2f', ax=axes[1])\n",
    "axes[1].set_title('Correlation Heatmap - Race')\n",
    "\n",
    "sns.heatmap(corr_rel_matrix, annot=True, cmap='coolwarm',\n",
    "            fmt='.2f', ax=axes[2])\n",
    "axes[2].set_title('Correlation Heatmap - Religion')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation for all columns together\n",
    "corr_gender_all = toxic_data.loc[t_gend, gender_cols + ['toxicity']]\n",
    "corr_race_all = toxic_data.loc[t_race, race_cols]\n",
    "corr_rel_all = toxic_data.loc[t_rel, religion_cols]\n",
    "\n",
    "all_data = pd.concat([corr_gender_all, corr_race_all, corr_rel_all], axis=1)\n",
    "\n",
    "# Calculate overall correlation matrix\n",
    "corr_all_data = all_data.corr()\n",
    "\n",
    "# Plot combined correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_all_data, annot=True, cmap='coolwarm', fmt='.2f',\n",
    "            annot_kws={\"size\": 8})\n",
    "plt.title('Combined Correlation Heatmap - All Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean toxicity for identity subgroups (all values taken into account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  gender columns\n",
    "mean_gender = df[gender_cols].mean()\n",
    "\n",
    "# race columns\n",
    "mean_race = df[race_cols].mean()\n",
    "\n",
    "# religion columns\n",
    "mean_religion = df[religion_cols].mean()\n",
    "\n",
    "# Print means\n",
    "print(\"Mean of Gender Columns:\")\n",
    "print(mean_gender)\n",
    "\n",
    "print(\"\\nMean of Race Columns:\")\n",
    "print(mean_race)\n",
    "\n",
    "print(\"\\nMean of Religion Columns:\")\n",
    "print(mean_religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gender = {\n",
    "    'male': 0.108047,\n",
    "    'female': 0.126652,\n",
    "    'bisexual': 0.001893,\n",
    "    'transgender': 0.006712,\n",
    "    'heterosexual': 0.003248,\n",
    "    'other_gender': 0.000882,\n",
    "    'homosexual_gay_or_lesbian': 0.025378,\n",
    "    'other_sexual_orientation': 0.001492\n",
    "}\n",
    "\n",
    "mean_race = {\n",
    "    'asian': 0.011886,\n",
    "    'black': 0.034276,\n",
    "    'white': 0.056535,\n",
    "    'latino': 0.006151,\n",
    "    'other_race_or_ethnicity': 0.008158\n",
    "}\n",
    "\n",
    "mean_religion = {\n",
    "    'hindu': 0.001443,\n",
    "    'buddhist': 0.001393,\n",
    "    'christian': 0.095184,\n",
    "    'muslim': 0.049078,\n",
    "    'jewish': 0.017910,\n",
    "    'atheist': 0.003468,\n",
    "    'other_religion': 0.006718\n",
    "}\n",
    "\n",
    "mean_gender_sorted1 = dict(sorted(mean_gender.items(),\n",
    "                                  key=lambda item: item[1]))\n",
    "mean_race_sorted1 = dict(sorted(mean_race.items(),\n",
    "                                key=lambda item: item[1]))\n",
    "mean_religion_sorted1 = dict(sorted(mean_religion.items(),\n",
    "                                    key=lambda item: item[1]))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(mean_gender_sorted1.keys(), mean_gender_sorted1.values(),\n",
    "        label='Gender', color='#9CC8ED', hatch='/')\n",
    "plt.bar(mean_race_sorted1.keys(), mean_race_sorted1.values(),\n",
    "        label='Race', color='#F4E8AB', hatch='o')\n",
    "plt.bar(mean_religion_sorted1.keys(), mean_religion_sorted1.values(),\n",
    "        label='Religion', color='#AAE2C3', hatch='.')\n",
    "\n",
    "plt.xlabel('Subgroups')\n",
    "plt.ylabel('Mean Values')\n",
    "plt.title('Mean Toxicity Scores for Different Subgroups')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "# Adding data labels to each bar\n",
    "for data in [mean_gender_sorted1, mean_race_sorted1, mean_religion_sorted1]:\n",
    "    for subgroup, value in data.items():\n",
    "        plt.text(subgroup, value + 0.001, round(value, 3), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean toxicity for identity subgroups (just values >= 0.5)\n",
    "\n",
    "@Purvi:  \n",
    "This plot IMO makes more sense than the previous one. Maybe drop previous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_toxic_gender = toxic_data[gender_cols]\\\n",
    "    .apply(lambda x: x[x >= 0.5].mean())\n",
    "mean_toxic_race = toxic_data[race_cols]\\\n",
    "    .apply(lambda x: x[x >= 0.5].mean())\n",
    "mean_toxic_religion = toxic_data[religion_cols]\\\n",
    "    .apply(lambda x: x[x >= 0.5].mean())\n",
    "\n",
    "# print means\n",
    "\n",
    "print(\"Mean of Gender Columns (>= 0.5 where toxic is 1):\")\n",
    "print(mean_toxic_gender)\n",
    "\n",
    "print(\"\\nMean of Race Columns (>= 0.5 where toxic is 1):\")\n",
    "print(mean_toxic_race)\n",
    "\n",
    "print(\"\\nMean of Religion Columns (>= 0.5 where toxic is 1):\")\n",
    "print(mean_toxic_religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gender = {\n",
    "    'male': 0.832137,\n",
    "    'female': 0.890529,\n",
    "    'bisexual': 0.683539,\n",
    "    'transgender': 0.821347,\n",
    "    'heterosexual': 0.747159,\n",
    "    'other_gender': 0.533333,\n",
    "    'homosexual_gay_or_lesbian': 0.878025,\n",
    "    'other_sexual_orientation': 0.533333\n",
    "}\n",
    "\n",
    "mean_race = {\n",
    "    'asian': 0.758636,\n",
    "    'black': 0.898275,\n",
    "    'white': 0.896285,\n",
    "    'latino': 0.729187,\n",
    "    'other_race_or_ethnicity': 0.567602\n",
    "}\n",
    "\n",
    "mean_religion = {\n",
    "    'hindu': 0.762626,\n",
    "    'buddhist': 0.731650,\n",
    "    'christian': 0.864552,\n",
    "    'muslim': 0.905658,\n",
    "    'jewish': 0.896070,\n",
    "    'atheist': 0.847757,\n",
    "    'other_religion': 0.528125\n",
    "}\n",
    "\n",
    "# sort in ascending order\n",
    "\n",
    "mean_gender_sorted = dict(sorted(mean_gender.items(),\n",
    "                                 key=lambda item: item[1]))\n",
    "mean_race_sorted = dict(sorted(mean_race.items(),\n",
    "                               key=lambda item: item[1]))\n",
    "mean_religion_sorted = dict(sorted(mean_religion.items(),\n",
    "                                   key=lambda item: item[1]))\n",
    "\n",
    "# plot\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(mean_gender_sorted.keys(), mean_gender_sorted.values(),\n",
    "        label='Gender', color='#9CC8ED', hatch='/')\n",
    "plt.bar(mean_race_sorted.keys(), mean_race_sorted.values(),\n",
    "        label='Race', color='#F4E8AB', hatch='o')\n",
    "plt.bar(mean_religion_sorted.keys(), mean_religion_sorted.values(),\n",
    "        label='Religion', color='#AAE2C3', hatch='.')\n",
    "\n",
    "plt.xlabel('Subgroups')\n",
    "plt.ylabel('Mean Values')\n",
    "plt.title('Mean Toxicity Scores for Different Subgroups')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "# Add data labels to each bar\n",
    "for data in [mean_gender, mean_race, mean_religion]:\n",
    "    for subgroup, value in data.items():\n",
    "        plt.text(subgroup, value + 0.001, round(value, 3), ha='center',\n",
    "                 va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('mean_toxicity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency for identity subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_gender = toxic_data.loc[t_gend, gender_cols + ['comment_text']]\n",
    "toxic_race = toxic_data.loc[t_race, race_cols + ['comment_text']]\n",
    "toxic_religion = toxic_data.loc[t_rel, religion_cols + ['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def most_common_words(data):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "    all_words = ' '.join(data['comment_text']).lower()\n",
    "    \n",
    "    # patterns to remove\n",
    "    additional_exclusions = ['’', '...', 's', \"n't\", 'get', 'one', 'would']  \n",
    "    \n",
    "    # remove punctuations\n",
    "    pattern = re.compile(r'[^\\w\\s]')\n",
    "    \n",
    "    words = word_tokenize(all_words)\n",
    "    # Remove stopwords, punctuation, and apply stemming\n",
    "    filtered_words = [\n",
    "        stemmer.stem(word)\n",
    "        for word in words\n",
    "        if word not in stop_words\n",
    "        and word not in string.punctuation\n",
    "        and word not in additional_exclusions\n",
    "        and word not in ['like']  # Exclude specific word 'like'\n",
    "        and not pattern.match(word)\n",
    "    ]\n",
    "    words_count = Counter(filtered_words)\n",
    "    return words_count.most_common(10)  # Get the 10 most common words\n",
    "\n",
    "# Get the most common stemmed words for each subgroup\n",
    "most_common_gender = most_common_words(toxic_gender)\n",
    "most_common_race = most_common_words(toxic_race)\n",
    "most_common_religion = most_common_words(toxic_religion)\n",
    "\n",
    "# Prepare data for plotting\n",
    "common_words = {\n",
    "    'Gender': most_common_gender,\n",
    "    'Race': most_common_race,\n",
    "    'Religion': most_common_religion\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = {'Gender': '#9CC8ED', 'Race': '#F4E8AB', 'Religion': '#AAE2C3'}\n",
    "\n",
    "for subgroup, common_words in common_words.items():\n",
    "    words, counts = zip(*common_words)\n",
    "    plt.barh([f'{subgroup}: {word}' for word in words], counts, label=subgroup,color=colors[subgroup])\n",
    "\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 10 Most Common Stemmed Words in Toxic Comments for \\\n",
    "          Different Subgroups')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('most_common.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations (André)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings: approved vs. rejected (bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of occurences of each identity\n",
    "counts = df['rating'].value_counts()\n",
    "\n",
    "# Plotting the distribution of the ratings\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(counts.index, counts.values, color=['green', 'red'])\n",
    "plt.xticks(counts.index, ['Approved', 'Rejected'])\n",
    "plt.title('Distribution of Approved vs Rejected Ratings')\n",
    "plt.ylabel('Number of Comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of data over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy original df in order to modify it\n",
    "df_time = df\n",
    "\n",
    "# @André:\n",
    "# Not sure: What's the meaning of the 2nd .str in the following line?\n",
    "# Is it necessary?\n",
    "df_time['date'] = df_time['created_date'].str.split(' ').str[0]\n",
    "df_time['date'] = pd.to_datetime(df_time['date'])\n",
    "\n",
    "# sort df by date\n",
    "df_time.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toxic comments over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.lineplot(x='date', y='toxic', data=df_time)\n",
    "plt.title('Toxic comments over time')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Toxic comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily percentage of toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only toxic comments per day\n",
    "daily_toxic_counts = df_time[df_time['toxic'] == 1].groupby('date').size()\n",
    "\n",
    "# get total comments per day\n",
    "\n",
    "# @André:\n",
    "# Just an idea, maybe I'm wrong: Isn't your next line equivalent to\n",
    "# df_time.groupby('date').size()\n",
    "# ? Maybe you would like to try it out because it's simpler.\n",
    "\n",
    "daily_counts = df_time['toxic'].groupby(df_time['date']).size()\n",
    "\n",
    "# % of toxic comments per day\n",
    "daily_percentage = (daily_toxic_counts / daily_counts) * 100\n",
    "\n",
    "# Plot the distribution of toxic comments over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "daily_percentage.plot(kind='bar', color='red', alpha=0.7)\n",
    "\n",
    "# Manually set x-axis labels to display every 7 days\n",
    "plt.xticks(range(0, len(daily_toxic_counts), 20),\n",
    "           [str(date.date()) for date in daily_toxic_counts.index[::20]],\n",
    "           rotation=45)\n",
    "plt.title('Daily % of Toxic Comments')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('% Toxic Comments')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily total of toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "daily_toxic_counts.plot(kind='bar', color='red', alpha=0.7)\n",
    "\n",
    "# Manually set x-axis labels to display every 7 days\n",
    "plt.xticks(range(0, len(daily_toxic_counts), 20),\n",
    "           [str(date.date()) for date in daily_toxic_counts.index[::20]],\n",
    "           rotation=45)\n",
    "plt.title('Daily Total of Toxic Comments')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Toxic Comments')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of reactions (as contained in Civil Comments metadata)\n",
    "Analyze how users react ('likes' or 'disagree') to toxic vs non-toxic comments. Reactions: 'funny', 'wow', 'sad', 'likes', 'disagree'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df containing just reaction cols\n",
    "df_react = df.loc[:, ('funny', 'wow', 'sad', 'likes', 'disagree', 'toxic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of comments with any reaction\n",
    "df_react['any_reaction'] = (df_react[['funny', 'wow', 'sad',\n",
    "                                      'likes', 'disagree']] > 0).any(axis=1)\n",
    "total_comments_with_reaction = df_react['any_reaction'].sum()\n",
    "\n",
    "# Calculate the percentage of comments with any reaction\n",
    "percentage_comments_with_reaction =\\\n",
    "    (total_comments_with_reaction / len(df_react)) * 100\n",
    "\n",
    "# Print the percentage\n",
    "print(f\"Percentage of comments with any reaction: {percentage_comments_with_reaction:.2f}%\")\n",
    "\n",
    "# Plot the percentage\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=['Comments with Reaction', 'Comments without Reaction'],\n",
    "            y=[percentage_comments_with_reaction,\n",
    "               100 - percentage_comments_with_reaction],\n",
    "            palette=['lightblue', 'lightgrey'])\n",
    "plt.title('Percentage of Comments with Any Reaction')\n",
    "plt.ylabel('Percentage');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of comments with any reaction\n",
    "df_react['any_reaction'] = (df_react[['funny', 'wow', 'sad',\n",
    "                                      'likes', 'disagree']] > 0).any(axis=1)\n",
    "\n",
    "# Create a contingency table to count occurrences\n",
    "contingency_table = pd.crosstab(df_react['toxic'], df_react['any_reaction'],\n",
    "                                margins=True, margins_name=\"Total\")\n",
    "\n",
    "# Calculate percentages\n",
    "percentage_comments_with_reaction_and_toxic = \\\n",
    "    (contingency_table[True] / contingency_table['Total']) * 100\n",
    "percentage_comments_without_reaction_and_toxic = \\\n",
    "    (contingency_table[False] / contingency_table['Total']) * 100\n",
    "\n",
    "# Print percentages\n",
    "print(\"Percentage of comments with reactions by toxicity:\")\n",
    "print(f\"With toxicity: {percentage_comments_with_reaction_and_toxic[True]:.2f}%\")\n",
    "print(f\"Without toxicity: {percentage_comments_without_reaction_and_toxic[True]:.2f}%\")\n",
    "\n",
    "# Plot the percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=contingency_table.index,\n",
    "            y=percentage_comments_with_reaction_and_toxic, color='lightgray',\n",
    "            label='With Reaction')\n",
    "sns.barplot(x=contingency_table.index,\n",
    "            y=percentage_comments_without_reaction_and_toxic, color='orange',\n",
    "            label='Without Reaction',\n",
    "            bottom=percentage_comments_with_reaction_and_toxic)\n",
    "\n",
    "plt.title('Percentage of Comments with and without Reactions by Toxicity')\n",
    "plt.xlabel('Toxicity')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_react_with_reaction = \\\n",
    "    df_react[df_react[['funny', 'wow', 'sad','likes', 'disagree']].sum(axis=1) > 0]\n",
    "\n",
    "reactions = ['funny', 'wow', 'sad', 'likes', 'disagree']\n",
    "\n",
    "for reaction in reactions:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(data=df_react_with_reaction, x='toxic',\n",
    "                y=reaction, errorbar=None)\n",
    "    plt.title(f'Grouped Bar Chart of {reaction.capitalize()} Reactions for Comments with Reactions')\n",
    "    plt.xlabel('Toxicity')\n",
    "    plt.ylabel(f'{reaction.capitalize()} Reactions')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee5ce8695951d3743acb1574d7cbc518a435066b16546d59d44a9748127e061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
