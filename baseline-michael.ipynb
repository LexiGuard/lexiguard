{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChnD36qHhuy"
      },
      "source": [
        "# Baseline model on final data (Michael)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTUtsD6HupB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisbn3mTG0fj",
        "outputId": "c3a65a47-0bea-4de2-c594-1b5ca57dec48"
      },
      "outputs": [],
      "source": [
        "# import the usual suspects / basics\n",
        "import time; full_run_time_start = time.time() # start timing exec right away\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy import sparse\n",
        "import re\n",
        "import os\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, f1_score,\\\n",
        "    accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# display all df columns (default is 20)\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility functions for testing models and tracking results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# empty df for storing results\n",
        "test_results = pd.DataFrame(columns=['model_name',\n",
        "                                'model_params',\n",
        "                                'data_desc',\n",
        "                                'data_size',\n",
        "                                'features_no',\n",
        "                                'f1',\n",
        "                                'acc',\n",
        "                                'recall',\n",
        "                                'prec',\n",
        "                                'roc_auc',\n",
        "                                'cf_matrix',\n",
        "                                'train_time',\n",
        "                                'notes'])\n",
        "\n",
        "def test_model(model, model_name, model_params, data_desc, X, y, notes=''):\n",
        "    '''\n",
        "    test_model(model, model_params, data_desc, X, y, notes='')\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: instance of model to test\n",
        "    model_name: name of model\n",
        "    model_params: dict of (hyper)parameters passed to model\n",
        "    data_desc: description of dataset (preprocessing steps etc.)\n",
        "    X: feature array \n",
        "    y: target/label array\n",
        "    notes: additional notes (default: empty string)\n",
        "    '''\n",
        "\n",
        "    # Split data using default of 75% for train, 25% for test.\n",
        "    # Make sure test data has same toxic/nontoxic ratio as train data by\n",
        "    # using stratify parameter.\n",
        "    X_train, X_test, y_train, y_test =\\\n",
        "        train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    \n",
        "    # train model and time execution\n",
        "    train_time_start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - train_time_start\n",
        "    train_time_str = f'{int(train_time // 60)}m {round(train_time % 60)}s'\n",
        "\n",
        "    # Make predictions on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    return {'model_name': model_name,\n",
        "            'model_params': model_params,\n",
        "            'data_desc': data_desc,\n",
        "            'data_size': X.shape[0],\n",
        "            'features_no': X.shape[1],\n",
        "            'f1': round(f1_score(y_test, y_pred), 5),\n",
        "            'acc': round(accuracy_score(y_test, y_pred), 5),\n",
        "            'recall': round(recall_score(y_test, y_pred), 5),\n",
        "            'prec': round(precision_score(y_test, y_pred), 5),\n",
        "            'roc_auc': round(roc_auc_score(y_test, y_pred_proba), 5),\n",
        "            'cf_matrix': confusion_matrix(y_test, y_pred),\n",
        "            'train_time': train_time_str,\n",
        "            'notes': notes}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_test_result(result):\n",
        "    test_results.loc[len(test_results)] = result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MP847vfIJMN"
      },
      "source": [
        "## Load data (final data file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r6YNY0NIIL4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(360301, 6)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('data/data_usampl_60_40_FINAL.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for NaN's ...\n",
            "raw                      0\n",
            "clean                  232\n",
            "clean_pp               236\n",
            "clean_pp_lemma         236\n",
            "clean_pp_lemma_stop    280\n",
            "toxic                    0\n",
            "dtype: int64\n",
            "\n",
            "Rows before dropping: 360301\n",
            "Rows after: 360021\n",
            "Rows dropped: 280\n"
          ]
        }
      ],
      "source": [
        "print('Checking for NaN\\'s ...')\n",
        "print(df.isna().sum())\n",
        "rows_before = df.shape[0]\n",
        "print(\"\\nRows before dropping:\", rows_before)\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "rows_after = df.shape[0]\n",
        "print('Rows after:', rows_after)\n",
        "print('Rows dropped:', rows_before - rows_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lZffM2npRCPf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw</th>\n",
              "      <th>clean</th>\n",
              "      <th>clean_pp</th>\n",
              "      <th>clean_pp_lemma</th>\n",
              "      <th>clean_pp_lemma_stop</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well, what are the chances he will turn out to...</td>\n",
              "      <td>Well, what are the chances he will turn out to...</td>\n",
              "      <td>well what are the chances he will turn out to ...</td>\n",
              "      <td>well what be the chance he will turn out to ha...</td>\n",
              "      <td>chance turn active proponent slavery</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The moment of critical mass is approaching whe...</td>\n",
              "      <td>The moment of critical mass is approaching whe...</td>\n",
              "      <td>the moment of critical mass is approaching whe...</td>\n",
              "      <td>the moment of critical mass be approach when t...</td>\n",
              "      <td>moment critical mass approach deed gupta co li...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Hey listen to me,\" he said. \"I'm not going to...</td>\n",
              "      <td>\"Hey listen to me,\" he said. \"I'm not going to...</td>\n",
              "      <td>hey listen to me he said i 'm not going to put...</td>\n",
              "      <td>hey listen to i he say i be not go to put up w...</td>\n",
              "      <td>hey listen say go crap prove reporter say uh a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We are already owed $488 M plus interest($2Bil...</td>\n",
              "      <td>We are already owed $ M plus interest($ Billio...</td>\n",
              "      <td>we are already owed $ m plus interest($ billio...</td>\n",
              "      <td>we be already owe $ m plus interest($ billion ...</td>\n",
              "      <td>owe $ m plus interest($ billion audits state c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>There is a reason there are no teeth to the la...</td>\n",
              "      <td>There is a reason there are no teeth to the la...</td>\n",
              "      <td>there is a reason there are no teeth to the la...</td>\n",
              "      <td>there be a reason there be no tooth to the law...</td>\n",
              "      <td>reason tooth law unlawful law way force free e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 raw  \\\n",
              "0  Well, what are the chances he will turn out to...   \n",
              "1  The moment of critical mass is approaching whe...   \n",
              "2  \"Hey listen to me,\" he said. \"I'm not going to...   \n",
              "3  We are already owed $488 M plus interest($2Bil...   \n",
              "4  There is a reason there are no teeth to the la...   \n",
              "\n",
              "                                               clean  \\\n",
              "0  Well, what are the chances he will turn out to...   \n",
              "1  The moment of critical mass is approaching whe...   \n",
              "2  \"Hey listen to me,\" he said. \"I'm not going to...   \n",
              "3  We are already owed $ M plus interest($ Billio...   \n",
              "4  There is a reason there are no teeth to the la...   \n",
              "\n",
              "                                            clean_pp  \\\n",
              "0  well what are the chances he will turn out to ...   \n",
              "1  the moment of critical mass is approaching whe...   \n",
              "2  hey listen to me he said i 'm not going to put...   \n",
              "3  we are already owed $ m plus interest($ billio...   \n",
              "4  there is a reason there are no teeth to the la...   \n",
              "\n",
              "                                      clean_pp_lemma  \\\n",
              "0  well what be the chance he will turn out to ha...   \n",
              "1  the moment of critical mass be approach when t...   \n",
              "2  hey listen to i he say i be not go to put up w...   \n",
              "3  we be already owe $ m plus interest($ billion ...   \n",
              "4  there be a reason there be no tooth to the law...   \n",
              "\n",
              "                                 clean_pp_lemma_stop  toxic  \n",
              "0               chance turn active proponent slavery      0  \n",
              "1  moment critical mass approach deed gupta co li...      0  \n",
              "2  hey listen say go crap prove reporter say uh a...      1  \n",
              "3  owe $ m plus interest($ billion audits state c...      0  \n",
              "4  reason tooth law unlawful law way force free e...      0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Create smaller sample from data to speed up experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using full data (360021 rows).\n"
          ]
        }
      ],
      "source": [
        "sample_size = None\n",
        "\n",
        "# uncomment to create sample of desired size\n",
        "#sample_size = 50_000\n",
        "\n",
        "if sample_size != None:\n",
        "    # ratio toxic/nontoxic\n",
        "    tox_perc = 0.4\n",
        "    nontox_perc = 0.6\n",
        "\n",
        "    # number of toxic/nontoxic rows\n",
        "    sample_size_tox = int(sample_size * tox_perc)\n",
        "    sample_size_nontox = int(sample_size * nontox_perc)\n",
        "\n",
        "    sample_tox = df[df['toxic'] == 1].sample(sample_size_tox,\n",
        "                                             random_state=42)\n",
        "    sample_nontox = df[df['toxic'] == 0].sample(sample_size_nontox,\n",
        "                                                random_state=42)\n",
        "\n",
        "    df = pd.concat([sample_tox, sample_nontox])\n",
        "    print(f'Using sample ({df.shape[0]} rows).')\n",
        "\n",
        "else:\n",
        "    print(f'Using full data ({df.shape[0]} rows).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create label/target variable and check for imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "29jE5PSrPFE2"
      },
      "outputs": [],
      "source": [
        "target = df['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nontoxic (0): 215687 (59.9 %)\n",
            "Toxic (1): 144334 (40.1 %)\n"
          ]
        }
      ],
      "source": [
        "value_counts = target.value_counts()\n",
        "nontoxic_count = value_counts[0]\n",
        "toxic_count = value_counts[1]\n",
        "nontoxic_perc =\\\n",
        "    round((nontoxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
        "toxic_perc =\\\n",
        "    round((toxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
        "\n",
        "print(f'Nontoxic (0): {nontoxic_count} ({nontoxic_perc} %)')\n",
        "print(f'Toxic (1): {toxic_count} ({toxic_perc} %)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function for bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bow(data):\n",
        "    vect = CountVectorizer()\n",
        "    return vect.fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run baseline model (logistic regression) on different data cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'max_iter': 2_000}\n",
        "\n",
        "# load model with parameters\n",
        "lr = LogisticRegression(**params)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words on col \"raw\"', bow(df['raw']), target)\n",
        "store_test_result(test_result)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words on col \"clean\"', bow(df['clean']), target)\n",
        "store_test_result(test_result)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words on col \"clean_pp\"', bow(df['clean_pp']), target)\n",
        "store_test_result(test_result)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words on col \"clean_pp_lemma\"', bow(df['clean_pp_lemma']), target)\n",
        "store_test_result(test_result)\n",
        "\n",
        "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
        "                    'bag of words on col \"clean_pp_lemma_stop\"', bow(df['clean_pp_lemma_stop']), target)\n",
        "store_test_result(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show test results + total exec time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_params</th>\n",
              "      <th>data_desc</th>\n",
              "      <th>data_size</th>\n",
              "      <th>features_no</th>\n",
              "      <th>f1</th>\n",
              "      <th>acc</th>\n",
              "      <th>recall</th>\n",
              "      <th>prec</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>cf_matrix</th>\n",
              "      <th>train_time</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BASELINE (logistic regression)</td>\n",
              "      <td>{'max_iter': 2000}</td>\n",
              "      <td>bag of words on col \"raw\"</td>\n",
              "      <td>360021</td>\n",
              "      <td>136663</td>\n",
              "      <td>0.82466</td>\n",
              "      <td>0.86619</td>\n",
              "      <td>0.78491</td>\n",
              "      <td>0.86866</td>\n",
              "      <td>0.92712</td>\n",
              "      <td>[[39712, 3426], [6209, 22658]]</td>\n",
              "      <td>0m 45s</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BASELINE (logistic regression)</td>\n",
              "      <td>{'max_iter': 2000}</td>\n",
              "      <td>bag of words on col \"clean\"</td>\n",
              "      <td>360021</td>\n",
              "      <td>122584</td>\n",
              "      <td>0.82506</td>\n",
              "      <td>0.86654</td>\n",
              "      <td>0.78501</td>\n",
              "      <td>0.86940</td>\n",
              "      <td>0.92745</td>\n",
              "      <td>[[39734, 3404], [6206, 22661]]</td>\n",
              "      <td>0m 40s</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BASELINE (logistic regression)</td>\n",
              "      <td>{'max_iter': 2000}</td>\n",
              "      <td>bag of words on col \"clean_pp\"</td>\n",
              "      <td>360021</td>\n",
              "      <td>122252</td>\n",
              "      <td>0.82499</td>\n",
              "      <td>0.86655</td>\n",
              "      <td>0.78460</td>\n",
              "      <td>0.86978</td>\n",
              "      <td>0.92748</td>\n",
              "      <td>[[39747, 3391], [6218, 22649]]</td>\n",
              "      <td>0m 42s</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BASELINE (logistic regression)</td>\n",
              "      <td>{'max_iter': 2000}</td>\n",
              "      <td>bag of words on col \"clean_pp_lemma\"</td>\n",
              "      <td>360021</td>\n",
              "      <td>108950</td>\n",
              "      <td>0.82336</td>\n",
              "      <td>0.86540</td>\n",
              "      <td>0.78249</td>\n",
              "      <td>0.86874</td>\n",
              "      <td>0.92858</td>\n",
              "      <td>[[39725, 3413], [6279, 22588]]</td>\n",
              "      <td>0m 46s</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BASELINE (logistic regression)</td>\n",
              "      <td>{'max_iter': 2000}</td>\n",
              "      <td>bag of words on col \"clean_pp_lemma_stop\"</td>\n",
              "      <td>360021</td>\n",
              "      <td>108910</td>\n",
              "      <td>0.82073</td>\n",
              "      <td>0.86359</td>\n",
              "      <td>0.77888</td>\n",
              "      <td>0.86734</td>\n",
              "      <td>0.92744</td>\n",
              "      <td>[[39699, 3439], [6383, 22484]]</td>\n",
              "      <td>0m 11s</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       model_name        model_params  \\\n",
              "0  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
              "1  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
              "2  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
              "3  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
              "4  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
              "\n",
              "                                   data_desc  data_size  features_no       f1  \\\n",
              "0                  bag of words on col \"raw\"     360021       136663  0.82466   \n",
              "1                bag of words on col \"clean\"     360021       122584  0.82506   \n",
              "2             bag of words on col \"clean_pp\"     360021       122252  0.82499   \n",
              "3       bag of words on col \"clean_pp_lemma\"     360021       108950  0.82336   \n",
              "4  bag of words on col \"clean_pp_lemma_stop\"     360021       108910  0.82073   \n",
              "\n",
              "       acc   recall     prec  roc_auc                       cf_matrix  \\\n",
              "0  0.86619  0.78491  0.86866  0.92712  [[39712, 3426], [6209, 22658]]   \n",
              "1  0.86654  0.78501  0.86940  0.92745  [[39734, 3404], [6206, 22661]]   \n",
              "2  0.86655  0.78460  0.86978  0.92748  [[39747, 3391], [6218, 22649]]   \n",
              "3  0.86540  0.78249  0.86874  0.92858  [[39725, 3413], [6279, 22588]]   \n",
              "4  0.86359  0.77888  0.86734  0.92744  [[39699, 3439], [6383, 22484]]   \n",
              "\n",
              "  train_time notes  \n",
              "0     0m 45s        \n",
              "1     0m 40s        \n",
              "2     0m 42s        \n",
              "3     0m 46s        \n",
              "4     0m 11s        "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full run time: 3m 41s\n"
          ]
        }
      ],
      "source": [
        "full_run_time = time.time() - full_run_time_start\n",
        "print(f'Full run time: {int(full_run_time // 60)}m {round(full_run_time % 60)}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate average comment length on cleaned data (before preproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average comment length:\n",
            "286 characters\n",
            "50 words\n"
          ]
        }
      ],
      "source": [
        "# characters\n",
        "comm_len_chars = df['clean'].apply(lambda s: len(s))\n",
        "avg_comm_len_chars = comm_len_chars.sum() / len(comm_len_chars)\n",
        "\n",
        "# words (rough count)\n",
        "comm_len_words = df['clean']\\\n",
        "    .apply(lambda s: len(re.findall(r'\\S+', s)))\n",
        "avg_comm_len_words = comm_len_words.sum() / len(comm_len_words)\n",
        "\n",
        "print('Average comment length:')\n",
        "print(round(avg_comm_len_chars), 'characters')\n",
        "print(round(avg_comm_len_words), 'words')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
